{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce6353a",
   "metadata": {
    "papermill": {
     "duration": 0.014238,
     "end_time": "2024-04-04T13:19:04.911778",
     "exception": false,
     "start_time": "2024-04-04T13:19:04.897540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Descripion\n",
    "\n",
    "# I made a mistake in posting the training notebook for the 82nd data set. Incorrect [4th version](https://www.kaggle.com/code/konstantinboyko/hms-resnet1d-gru-train-1-5-dataset?scriptVersionId=166379987). This is a replacement for version 4.\n",
    "\n",
    "## The notebook is based on the excellent version of the [HMS Resnet1D-GRU Train notebook by Med Ali Bouchhioua](https://www.kaggle.com/code/medali1992/hms-resnet1d-gru-train?scriptVersionId=163575181)\n",
    "\n",
    "## Changes 1 [LB:0.40]:\n",
    "\n",
    "- Convolution kernel used [3,5,7,9,11]\n",
    "- Loss functions replaced with Hardswish and SiLU\n",
    "- Adan optimizer replaced with AdamW\n",
    "- Bandpass filter with a lower limit of 0.5 Hz\n",
    "- Total Evaluators are used in the first data set from 0 to 5, the second data set from 6 to max\n",
    "- Albumentations. Random frequency cut with a bandpass filter in the range 10 - 25 Hz\n",
    "- 20 epochs with two stages\n",
    "\n",
    "## Changes 2 [LB:0.38]:\n",
    "- Order of filter changed from 6 to 2 and high cutoff frequency changed from 25 Hz to 20 Hz. An order with order 6 has a very strong effect on the signal if there are sharp jumps.\n",
    "\n",
    "## Changes 3 [LB:0.38]:\n",
    "- The total of appraisers is divided into three parts: [0..2], [3..5], [6..1000].\n",
    "\n",
    "## Changes 4 [LB:0.40]:\n",
    "- Return to total of appraisers is divided into two parts: [0..5], [6..1000].\n",
    "- Remove Regularization value 0.166666667 according to the advice of [Med Ali Bouchhioua](https://www.kaggle.com/code/konstantinboyko/hms-resnet1d-gru-v22-human-6-train/comments#2681934).\n",
    "- Added code that allows you to train the model not only in the stage/fold section, but also in the reverse fold/stage section.\n",
    "- Added filter parameters.\n",
    "\n",
    "## Changes 5 [LB:0.39]:\n",
    "- Albumentations. Accidentally missing an entire signal.\n",
    "\n",
    "## Changes 6 [LB:0.38]\n",
    "- The signal size is reduced by half and is selected randomly from the total signal.\n",
    "\n",
    "## Changes 7 [LB:0.36]\n",
    "- The signal size is reduced by five times and selected randomly from the total signal.\n",
    "\n",
    "## Changes 8 [LB:0.39]\n",
    "- One-stage model with the number of evaluators in the range [5..Max]\n",
    "\n",
    "## Changes 9 [LB:0.38]\n",
    "- The signal size is reduced by five times and selected randomly from the total signal.\n",
    "- Total Evaluators in the range [2..2 + 6..28]\n",
    "\n",
    "## Changes 11 [LB:0.37]\n",
    "- The total of appraisers is divided into two parts: [1..2 + 4..5], [6..28].\n",
    "\n",
    "## Changes 12 [LB:]\n",
    "- The total of appraisers is divided into two parts: [1..5 -4(GPD)], [6..28]\n",
    "\n",
    "## [Final 82nd Dataset](https://www.kaggle.com/datasets/konstantinboyko/hms-resnet1d-gru-weights-v82)\n",
    "\n",
    "## [Previous Train](https://www.kaggle.com/code/konstantinboyko/hms-resnet1d-gru-v33-human-5-stage-1-train)\n",
    "\n",
    "## [Inference Notebook for 82nd Dataset](https://www.kaggle.com/code/konstantinboyko/hms-resnet1d-gru-inference-1-5-dataset)\n",
    "\n",
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18868511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:04.940147Z",
     "iopub.status.busy": "2024-04-04T13:19:04.939789Z",
     "iopub.status.idle": "2024-04-04T13:19:13.157368Z",
     "shell.execute_reply": "2024-04-04T13:19:13.156218Z"
    },
    "papermill": {
     "duration": 8.234386,
     "end_time": "2024-04-04T13:19:13.159644",
     "exception": false,
     "start_time": "2024-04-04T13:19:04.925258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ubuntu 20.04.6 LTS\r\n",
      "BUILD_DATE=20240222-122512, CONTAINER_NAME=tf2-gpu/2-15+cu121\n",
      "PyTorch Version:2.1.2, CUDA is available:True, Version CUDA:12.1\n",
      "Device Capability:(6, 0), ['sm_60', 'sm_70', 'sm_75', 'compute_70', 'compute_75']\n",
      "CuDNN Enabled:True, Version:8900\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "import scipy.signal as scisig\n",
    "from scipy.signal import butter, lfilter, freqz\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import (\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    ")\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "#import cupy as cp\n",
    "#import cupyx.scipy.signal as cpsig\n",
    "\n",
    "sys.path.append(\"/kaggle/input/kaggle-kl-div\")\n",
    "import kaggle_kl_div\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "!cat /etc/os-release | grep -oP \"PRETTY_NAME=\\\"\\K([^\\\"]*)\"\n",
    "print(f\"BUILD_DATE={os.environ['BUILD_DATE']}, CONTAINER_NAME={os.environ['CONTAINER_NAME']}\")\n",
    "\n",
    "try:\n",
    "    print(\n",
    "        f\"PyTorch Version:{torch.__version__}, CUDA is available:{torch.cuda.is_available()}, Version CUDA:{torch.version.cuda}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Device Capability:{torch.cuda.get_device_capability()}, {torch.cuda.get_arch_list()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"CuDNN Enabled:{torch.backends.cudnn.enabled}, Version:{torch.backends.cudnn.version()}\"\n",
    "    )\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be162a",
   "metadata": {
    "papermill": {
     "duration": 0.013184,
     "end_time": "2024-04-04T13:19:13.186698",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.173514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da1189b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.216668Z",
     "iopub.status.busy": "2024-04-04T13:19:13.215555Z",
     "iopub.status.idle": "2024-04-04T13:19:13.226809Z",
     "shell.execute_reply": "2024-04-04T13:19:13.225931Z"
    },
    "papermill": {
     "duration": 0.028761,
     "end_time": "2024-04-04T13:19:13.228855",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.200094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter:True, kaggle:True, local:False\n",
      ".\n",
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "class APP:\n",
    "    jupyter = \"ipykernel\" in globals()\n",
    "    if not jupyter:\n",
    "        try:\n",
    "            if \"IPython\" in globals().get(\"__doc__\", \"\"):\n",
    "                jupyter = True\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "\n",
    "    kaggle = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\") != \"\"\n",
    "    local = os.environ.get(\"DOCKER_USING\", \"\") == \"LOCAL\"\n",
    "    date_time_start = dt.datetime.now()\n",
    "    dt_start_ymd_hms = date_time_start.strftime(\"%Y.%m.%d_%H-%M-%S\")\n",
    "\n",
    "    file_run_path = \"\"\n",
    "    if jupyter:\n",
    "        try:\n",
    "            file_run_path = Path(globals().get(\"__vsc_ipynb_file__\", \"\"))\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            file_run_path = Path(__file__)\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "\n",
    "    file_run_name = file_run_path.stem\n",
    "    path_app = file_run_path.parent\n",
    "    path_run = Path(os.getcwd())\n",
    "    path_out = (\n",
    "        Path(\"/kaggle/working\")\n",
    "        if kaggle\n",
    "        else file_run_path / f\"{file_run_name}_{dt_start_ymd_hms}\"\n",
    "    )\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"./\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "print(f\"jupyter:{APP.jupyter}, kaggle:{APP.kaggle}, local:{APP.local}\")\n",
    "print(APP.file_run_path)\n",
    "print(APP.path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d7c26",
   "metadata": {
    "papermill": {
     "duration": 0.013603,
     "end_time": "2024-04-04T13:19:13.256011",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.242408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2f1f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.284515Z",
     "iopub.status.busy": "2024-04-04T13:19:13.284252Z",
     "iopub.status.idle": "2024-04-04T13:19:13.309067Z",
     "shell.execute_reply": "2024-04-04T13:19:13.308216Z"
    },
    "papermill": {
     "duration": 0.041985,
     "end_time": "2024-04-04T13:19:13.311443",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.269458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fp1': 0, 'T3': 1, 'C3': 2, 'O1': 3, 'Fp2': 4, 'C4': 5, 'T4': 6, 'O2': 7}\n",
      "['Fp1', 'T3', 'C3', 'O1', 'Fp2', 'C4', 'T4', 'O2']\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    VERSION = '82'\n",
    "\n",
    "    wandb = False\n",
    "    debug = False\n",
    "    create_eegs = False\n",
    "    apex = True\n",
    "    visualize = False\n",
    "    save_all_models = True\n",
    "\n",
    "    if debug:\n",
    "        num_workers = 0\n",
    "        parallel = False\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "        parallel = True\n",
    "\n",
    "    model_name = \"resnet1d_gru\"\n",
    "#     optimizer = \"Adam\"\n",
    "#     optimizer = \"Adan\"\n",
    "    optimizer = \"AdamW\"\n",
    "\n",
    "    factor = 0.9\n",
    "    eps = 1e-6\n",
    "    lr = 8e-3\n",
    "    min_lr = 1e-6\n",
    "\n",
    "    batch_size = 64\n",
    "    batch_koef_valid = 2\n",
    "    batch_scheduler = True\n",
    "    weight_decay = 1e-2\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1e7\n",
    "\n",
    "    fixed_kernel_size = 5\n",
    "    # linear_layer_features = 424\n",
    "    # kernels = [3, 5, 7, 9]\n",
    "    #linear_layer_features = 448  # Full Signal = 10_000\n",
    "    #linear_layer_features = 352  # Half Signal = 5_000\n",
    "    linear_layer_features = 304   # 1/4, 1/5, 1/6  Signal = 2_000\n",
    "    #linear_layer_features = 280  # 1/10  Signal = 1_000\n",
    "    kernels = [3, 5, 7, 9, 11]\n",
    "    # kernels = [5, 7, 9, 11, 13]\n",
    "\n",
    "    seq_length = 50  # Second's\n",
    "    sampling_rate = 200  # Hz\n",
    "    nsamples = seq_length * sampling_rate  # Число семплов 10_000\n",
    "    n_split_samples = 5\n",
    "    out_samples = nsamples // n_split_samples  # 2_000\n",
    "    sample_delta = nsamples - out_samples  # 8000\n",
    "    sample_offset = sample_delta // 2\n",
    "    multi_validation = False\n",
    "\n",
    "    train_by_stages = False\n",
    "    train_by_folds = True\n",
    "\n",
    "    # 'GPD', 'GRDA', 'LPD', 'LRDA', 'Other', 'Seizure'\n",
    "    n_stages = 2\n",
    "    match n_stages:\n",
    "        case 1:\n",
    "            train_stages = [0]\n",
    "            epochs = [100]\n",
    "            test_total_eval = 2\n",
    "            total_evals_old = [[(2, 3), (6, 29)]]  # Deprecated\n",
    "            total_evaluators = [ \n",
    "                [   \n",
    "                    {'band':(2, 2), 'excl_evals':[]}, \n",
    "                    {'band':(6, 28), 'excl_evals':[]},\n",
    "                ], \n",
    "            ]            \n",
    "        case 2:\n",
    "            train_stages = [0, 1]\n",
    "            epochs = [50, 100]\n",
    "            test_total_eval = 4\n",
    "            total_evals_old = [[(1, 4),(4, 5), (5, 6)], (6, 29)]  # Deprecated\n",
    "            total_evaluators = [ \n",
    "                [   \n",
    "                    {'band':(1, 3), 'excl_evals':[]}, \n",
    "                    {'band':(4, 4), 'excl_evals':['GPD']}, \n",
    "                    {'band':(5, 5), 'excl_evals':[]}, \n",
    "                ], \n",
    "                [   \n",
    "                    {'band':(6, 28), 'excl_evals':[]},\n",
    "                ], \n",
    "            ]            \n",
    "        case 3:\n",
    "            train_stages = [0, 1, 2]\n",
    "            epochs = [20, 50, 100]\n",
    "            test_total_eval = 0\n",
    "            total_evals_old = [(0, 3), (3, 6), (6, 29)]  # Deprecated\n",
    "            total_evaluators = [ \n",
    "                [   \n",
    "                    {'band':(0, 2), 'excl_evals':[]}, \n",
    "                ], \n",
    "                [   \n",
    "                    {'band':(3, 5), 'excl_evals':[]}, \n",
    "                ], \n",
    "                [   \n",
    "                    {'band':(6, 28), 'excl_evals':[]},\n",
    "                ], \n",
    "            ]            \n",
    "    \n",
    "    n_fold = 5\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "    # train_folds = [0]\n",
    "\n",
    "    patience = 11\n",
    "    seed = 2024\n",
    "\n",
    "    bandpass_filter = {\"low\": 0.5, \"high\": 10, \"order\": 2}\n",
    "    rand_filter = {\"probab\": 0.1, \"low\": 10, \"high\": 20, \"band\": 1.0, \"order\": 2}\n",
    "    freq_channels = []  # [(8.0, 12.0)]; [(0.5, 4.5)]\n",
    "    filter_order = 2\n",
    "\n",
    "    random_divide_signal = 0.05\n",
    "    random_close_zone = 0.05\n",
    "    random_common_negative_signal = 0.0\n",
    "    random_common_reverse_signal = 0.0\n",
    "    random_negative_signal = 0.05\n",
    "    random_reverse_signal = 0.05\n",
    "\n",
    "    log_step = 100  # Шаг отображения тренировки\n",
    "    log_show = False\n",
    "\n",
    "    scheduler = \"CosineAnnealingWarmRestarts\"  # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts','OneCycleLR']\n",
    "\n",
    "    # CosineAnnealingLR params\n",
    "    cosanneal_params = {\n",
    "        \"T_max\": 6,\n",
    "        \"eta_min\": 1e-5,\n",
    "        \"last_epoch\": -1,\n",
    "    }\n",
    "\n",
    "    # ReduceLROnPlateau params\n",
    "    reduce_params = {\n",
    "        \"mode\": \"min\",\n",
    "        \"factor\": 0.2,\n",
    "        \"patience\": 4,\n",
    "        \"eps\": 1e-6,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "    # CosineAnnealingWarmRestarts params\n",
    "    cosanneal_res_params = {\n",
    "        \"T_0\": 20,\n",
    "        \"eta_min\": 1e-6,\n",
    "        \"T_mult\": 1,\n",
    "        \"last_epoch\": -1,\n",
    "    }\n",
    "\n",
    "    target_cols = [\n",
    "        \"seizure_vote\",\n",
    "        \"lpd_vote\",\n",
    "        \"gpd_vote\",\n",
    "        \"lrda_vote\",\n",
    "        \"grda_vote\",\n",
    "        \"other_vote\",\n",
    "    ]\n",
    "\n",
    "    pred_cols = [x + \"_pred\" for x in target_cols]\n",
    "\n",
    "    map_features = [\n",
    "        (\"Fp1\", \"T3\"),\n",
    "        (\"T3\", \"O1\"),\n",
    "        (\"Fp1\", \"C3\"),\n",
    "        (\"C3\", \"O1\"),\n",
    "        (\"Fp2\", \"C4\"),\n",
    "        (\"C4\", \"O2\"),\n",
    "        (\"Fp2\", \"T4\"),\n",
    "        (\"T4\", \"O2\"),\n",
    "        #('Fz', 'Cz'), ('Cz', 'Pz'),\n",
    "    ]\n",
    "\n",
    "    eeg_features = [\"Fp1\", \"T3\", \"C3\", \"O1\", \"Fp2\", \"C4\", \"T4\", \"O2\"]  # 'Fz', 'Cz', 'Pz'\n",
    "        # 'F3', 'P3', 'F7', 'T5', 'Fz', 'Cz', 'Pz', 'F4', 'P4', 'F8', 'T6', 'EKG']\n",
    "    feature_to_index = {x: y for x, y in zip(eeg_features, range(len(eeg_features)))}\n",
    "    simple_features = []  # 'Fz', 'Cz', 'Pz', 'EKG'\n",
    "\n",
    "    # eeg_features = [row for row in feature_to_index]\n",
    "    # eeg_feat_size = len(eeg_features)\n",
    "    \n",
    "    n_map_features = len(map_features)\n",
    "    in_channels = n_map_features + n_map_features * len(freq_channels) + len(simple_features)\n",
    "    target_size = len(target_cols)\n",
    "\n",
    "    path_inp = Path(\"/kaggle/input\")\n",
    "    path_src = path_inp / \"hms-harmful-brain-activity-classification/\"\n",
    "    file_train = path_src / \"train.csv\"\n",
    "    path_train = path_src / \"train_eegs\"\n",
    "    file_features_test = path_train / \"100261680.parquet\"\n",
    "    file_eeg_specs = path_inp / \"eeg-spectrogram-by-lead-id-unique/eeg_specs.npy\"\n",
    "    file_raw_eeg = path_inp / \"brain-eegs/eegs.npy\"\n",
    "    #file_raw_eeg = path_inp / \"brain-eegs-plus/eegs.npy\"\n",
    "    #file_raw_eeg = path_inp / \"brain-eegs-full/eegs.npy\"\n",
    "\n",
    "    if APP.kaggle:\n",
    "        num_workers = 2\n",
    "        parallel = True\n",
    "        # GPU_DEVICES = \"auto\"\n",
    "\n",
    "\n",
    "# print(CFG.eeg_feat_size, CFG.in_channels)\n",
    "print(CFG.feature_to_index)\n",
    "print(CFG.eeg_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed67284c",
   "metadata": {
    "papermill": {
     "duration": 0.013705,
     "end_time": "2024-04-04T13:19:13.339242",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.325537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0c794e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.368177Z",
     "iopub.status.busy": "2024-04-04T13:19:13.367866Z",
     "iopub.status.idle": "2024-04-04T13:19:13.383326Z",
     "shell.execute_reply": "2024-04-04T13:19:13.382661Z"
    },
    "papermill": {
     "duration": 0.032166,
     "end_time": "2024-04-04T13:19:13.385183",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.353017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def quantize_data(data, classes):\n",
    "    mu_x = mu_law_encoding(data, classes)\n",
    "    return mu_x  # quantized\n",
    "\n",
    "\n",
    "def mu_law_encoding(data, mu):\n",
    "    mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n",
    "    return mu_x\n",
    "\n",
    "\n",
    "def mu_law_expansion(data, mu):\n",
    "    s = np.sign(data) * (np.exp(np.abs(data) * np.log(mu + 1)) - 1) / mu\n",
    "    return s\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype=\"band\")\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def butter_lowpass_filter(\n",
    "    data, cutoff_freq=20, sampling_rate=CFG.sampling_rate, order=4\n",
    "):\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff_freq / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype=\"low\", analog=False)\n",
    "    filtered_data = lfilter(b, a, data, axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def denoise_filter(x):\n",
    "    # Частота дискретизации и желаемые частоты среза (в Гц).\n",
    "    # Отфильтруйте шумный сигнал\n",
    "    y = butter_bandpass_filter(x, CFG.lowcut, CFG.highcut, CFG.sampling_rate, order=6)\n",
    "    y = (y + np.roll(y, -1) + np.roll(y, -2) + np.roll(y, -3)) / 4\n",
    "    y = y[0:-1:4]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25aecb3",
   "metadata": {
    "papermill": {
     "duration": 0.014629,
     "end_time": "2024-04-04T13:19:13.413586",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.398957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Parquet to EEG Signals Numpy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8438fb8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.442549Z",
     "iopub.status.busy": "2024-04-04T13:19:13.442284Z",
     "iopub.status.idle": "2024-04-04T13:19:13.454007Z",
     "shell.execute_reply": "2024-04-04T13:19:13.453153Z"
    },
    "papermill": {
     "duration": 0.028535,
     "end_time": "2024-04-04T13:19:13.455871",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.427336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eeg_from_parquet(\n",
    "    parquet_path: str, display: bool = False, seq_length=CFG.seq_length\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Эта функция читает файл паркета и извлекает средние 50 секунд показаний. Затем он заполняет значения NaN\n",
    "    со средним значением (игнорируя NaN).\n",
    "        :param parquet_path: путь к файлу паркета.\n",
    "        :param display: отображать графики ЭЭГ или нет.\n",
    "        :return data: np.array формы (time_steps, eeg_features) -> (10_000, 8)\n",
    "    \"\"\"\n",
    "\n",
    "    # Вырезаем среднюю 50 секундную часть\n",
    "    eeg = pd.read_parquet(parquet_path, columns=CFG.eeg_features)\n",
    "    rows = len(eeg)\n",
    "\n",
    "    # начало смещения данных, чтобы забрать середину\n",
    "    offset = (rows - CFG.nsamples) // 2\n",
    "\n",
    "    # средние 50 секунд, имеет одинаковое количество показаний слева и справа\n",
    "    eeg = eeg.iloc[offset : offset + CFG.nsamples]\n",
    "\n",
    "    if display:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        offset = 0\n",
    "\n",
    "    # Конвертировать в numpy\n",
    "\n",
    "    # создать заполнитель той же формы с нулями\n",
    "    data = np.zeros((CFG.nsamples, len(CFG.eeg_features)))\n",
    "\n",
    "    for index, feature in enumerate(CFG.eeg_features):\n",
    "        x = eeg[feature].values.astype(\"float32\")  # конвертировать в float32\n",
    "\n",
    "        # Вычисляет среднее арифметическое вдоль указанной оси, игнорируя NaN.\n",
    "        mean = np.nanmean(x)\n",
    "        nan_percentage = np.isnan(x).mean()  # percentage of NaN values in feature\n",
    "\n",
    "        # Заполнение значения Nan\n",
    "        # Поэлементная проверка на NaN и возврат результата в виде логического массива.\n",
    "        if nan_percentage < 1:  # если некоторые значения равны Nan, но не все\n",
    "            x = np.nan_to_num(x, nan=mean)\n",
    "        else:  # если все значения — Nan\n",
    "            x[:] = 0\n",
    "        data[:, index] = x\n",
    "\n",
    "        if display:\n",
    "            if index != 0:\n",
    "                offset += x.max()\n",
    "            plt.plot(range(CFG.nsamples), x - offset, label=feature)\n",
    "            offset -= x.min()\n",
    "\n",
    "    if display:\n",
    "        plt.legend()\n",
    "        name = parquet_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"EEG {name}\", size=16)\n",
    "        plt.show()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d465ee",
   "metadata": {
    "papermill": {
     "duration": 0.015242,
     "end_time": "2024-04-04T13:19:13.484971",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.469729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c73c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.515330Z",
     "iopub.status.busy": "2024-04-04T13:19:13.514656Z",
     "iopub.status.idle": "2024-04-04T13:19:13.549056Z",
     "shell.execute_reply": "2024-04-04T13:19:13.548034Z"
    },
    "papermill": {
     "duration": 0.052475,
     "end_time": "2024-04-04T13:19:13.551383",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.498908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        eegs: Dict[int, np.ndarray],\n",
    "        mode: str = \"train\",\n",
    "        downsample: int = None,\n",
    "        bandpass_filter: Dict[str, Union[int, float]] = None,\n",
    "        rand_filter: Dict[str, Union[int, float]] = None,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.eegs = eegs\n",
    "        self.downsample = downsample\n",
    "        self.offset = None\n",
    "        self.bandpass_filter = bandpass_filter\n",
    "        self.rand_filter = rand_filter\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length of dataset.\n",
    "        \"\"\"\n",
    "        # Обозначает количество пакетов за эпоху\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get one item.\n",
    "        \"\"\"\n",
    "        # Сгенерировать один пакет данных\n",
    "        X, y_prob = self.__data_generation(index)\n",
    "        if self.downsample is not None:\n",
    "            X = X[:: self.downsample, :]\n",
    "        output = {\n",
    "            \"eeg\": torch.tensor(X, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(y_prob, dtype=torch.float32),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def set_offset(self, offset: int):\n",
    "        self.offset = offset\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        # Генерирует данные, содержащие образцы размера партии\n",
    "        X = np.zeros(\n",
    "            (CFG.out_samples, CFG.in_channels), dtype=\"float32\"\n",
    "        )  # Size=(10000, 14)\n",
    "\n",
    "        random_divide_signal = False\n",
    "        row = self.df.iloc[index]  # Строка Pandas\n",
    "        data = self.eegs[row.eeg_id]  # Size=(10000, 8)\n",
    "        if CFG.nsamples != CFG.out_samples:\n",
    "            if self.mode == \"train\":\n",
    "                offset = (CFG.sample_delta * random.randint(0, 1000)) // 1000\n",
    "            elif not self.offset is None:\n",
    "                offset = self.offset\n",
    "            else:\n",
    "                offset = CFG.sample_offset\n",
    "\n",
    "            if self.mode == \"train\" and CFG.random_divide_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_divide_signal:\n",
    "                random_divide_signal = True\n",
    "                multipliers = [(1, 2), (2, 3), (3, 4), (3, 5)]\n",
    "                koef_1, koef_2 = multipliers[random.randint(0, 3)]\n",
    "                offset = (koef_1 * offset) // koef_2\n",
    "                data = data[offset:offset+(CFG.out_samples * koef_2) // koef_1,:]\n",
    "            else:\n",
    "                data = data[offset:offset+CFG.out_samples,:]\n",
    "\n",
    "        reverse_signal = False\n",
    "        negative_signal = False\n",
    "        if self.mode == \"train\":\n",
    "            if CFG.random_common_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_reverse_signal:\n",
    "                reverse_signal = True\n",
    "            if CFG.random_common_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_negative_signal:\n",
    "                negative_signal = True\n",
    "\n",
    "        for i, (feat_a, feat_b) in enumerate(CFG.map_features):\n",
    "            if self.mode == \"train\" and CFG.random_close_zone > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_close_zone:\n",
    "                continue\n",
    "            \n",
    "            diff_feat = (\n",
    "                data[:, CFG.feature_to_index[feat_a]]\n",
    "                - data[:, CFG.feature_to_index[feat_b]]\n",
    "            )  # Size=(10000,)\n",
    "\n",
    "            if self.mode == \"train\":\n",
    "                if reverse_signal or CFG.random_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_reverse_signal:\n",
    "                    diff_feat = np.flip(diff_feat)\n",
    "                if negative_signal or CFG.random_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_negative_signal:\n",
    "                    diff_feat = -diff_feat\n",
    "\n",
    "            if not self.bandpass_filter is None:\n",
    "                diff_feat = butter_bandpass_filter(\n",
    "                    diff_feat,\n",
    "                    self.bandpass_filter[\"low\"],\n",
    "                    self.bandpass_filter[\"high\"],\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.bandpass_filter[\"order\"],\n",
    "                )\n",
    "            \n",
    "            if random_divide_signal:\n",
    "                #diff_feat = cp.asnumpy(cpsig.upfirdn([1.0, 1, 1.0], diff_feat, 2, 3))  # linear interp, rate 2/3\n",
    "                diff_feat = scisig.upfirdn([1.0, 1, 1.0], diff_feat, koef_1, koef_2)  # linear interp, rate 2/3\n",
    "                diff_feat = diff_feat[0:CFG.out_samples]\n",
    "\n",
    "            if (\n",
    "                self.mode == \"train\"\n",
    "                and not self.rand_filter is None\n",
    "                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n",
    "            ):\n",
    "                lowcut = random.randint(\n",
    "                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n",
    "                )\n",
    "                highcut = lowcut + self.rand_filter[\"band\"]\n",
    "                diff_feat = butter_bandpass_filter(\n",
    "                    diff_feat,\n",
    "                    lowcut,\n",
    "                    highcut,\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.rand_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            X[:, i] = diff_feat\n",
    "\n",
    "        n = CFG.n_map_features\n",
    "        if len(CFG.freq_channels) > 0:\n",
    "            for i in range(CFG.n_map_features):\n",
    "                diff_feat = X[:, i]\n",
    "                for j, (lowcut, highcut) in enumerate(CFG.freq_channels):\n",
    "                    band_feat = butter_bandpass_filter(\n",
    "                        diff_feat, lowcut, highcut, CFG.sampling_rate, order=CFG.filter_order,  # 6\n",
    "                    )\n",
    "                    X[:, n] = band_feat\n",
    "                    n += 1\n",
    "\n",
    "        for spml_feat in CFG.simple_features:\n",
    "            feat_val = data[:, CFG.feature_to_index[spml_feat]]\n",
    "            \n",
    "            if not self.bandpass_filter is None:\n",
    "                feat_val = butter_bandpass_filter(\n",
    "                    feat_val,\n",
    "                    self.bandpass_filter[\"low\"],\n",
    "                    self.bandpass_filter[\"high\"],\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.bandpass_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                self.mode == \"train\"\n",
    "                and not self.rand_filter is None\n",
    "                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n",
    "            ):\n",
    "                lowcut = random.randint(\n",
    "                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n",
    "                )\n",
    "                highcut = lowcut + self.rand_filter[\"band\"]\n",
    "                feat_val = butter_bandpass_filter(\n",
    "                    feat_val,\n",
    "                    lowcut,\n",
    "                    highcut,\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.rand_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            X[:, n] = feat_val\n",
    "            n += 1\n",
    "            \n",
    "        # Обрезать края превышающие значения [-1024, 1024]\n",
    "        X = np.clip(X, -1024, 1024)\n",
    "\n",
    "        # Замените NaN нулем и разделить все на 32\n",
    "        X = np.nan_to_num(X, nan=0) / 32.0\n",
    "\n",
    "        # обрезать полосовым фильтром верхнюю границу в 20 Hz.\n",
    "        X = butter_lowpass_filter(X, order=CFG.filter_order)  # 4\n",
    "\n",
    "        y_prob = np.zeros(CFG.target_size, dtype=\"float32\")  # Size=(6,)\n",
    "        if self.mode != \"test\":\n",
    "            y_prob = row[CFG.target_cols].values.astype(np.float32)\n",
    "\n",
    "        return X, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81e28f",
   "metadata": {
    "papermill": {
     "duration": 0.015011,
     "end_time": "2024-04-04T13:19:13.582318",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.567307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1709a900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.617079Z",
     "iopub.status.busy": "2024-04-04T13:19:13.616208Z",
     "iopub.status.idle": "2024-04-04T13:19:13.626977Z",
     "shell.execute_reply": "2024-04-04T13:19:13.626000Z"
    },
    "papermill": {
     "duration": 0.030532,
     "end_time": "2024-04-04T13:19:13.629087",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.598555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KLDivLossWithLogits(nn.KLDivLoss):\n",
    "    def __init__(self):\n",
    "        super().__init__(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        y = nn.functional.log_softmax(y, dim=1)\n",
    "        loss = super().forward(y, t)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True  # Это опция требует много паямяти GPU\n",
    "    # pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565fa90e",
   "metadata": {
    "papermill": {
     "duration": 0.015872,
     "end_time": "2024-04-04T13:19:13.660555",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.644683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55641ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.693625Z",
     "iopub.status.busy": "2024-04-04T13:19:13.693314Z",
     "iopub.status.idle": "2024-04-04T13:19:13.722736Z",
     "shell.execute_reply": "2024-04-04T13:19:13.721861Z"
    },
    "papermill": {
     "duration": 0.04856,
     "end_time": "2024-04-04T13:19:13.724898",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.676338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet_1D_Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        downsampling,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super(ResNet_1D_Block, self).__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=in_channels)\n",
    "        # self.relu = nn.ReLU(inplace=False)\n",
    "        # self.relu_1 = nn.PReLU()\n",
    "        # self.relu_2 = nn.PReLU()\n",
    "        self.relu_1 = nn.Hardswish()\n",
    "        self.relu_2 = nn.Hardswish()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout, inplace=False)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "        )\n",
    "        self.downsampling = downsampling\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.maxpool(out)\n",
    "        identity = self.downsampling(x)\n",
    "\n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernels,\n",
    "        in_channels,\n",
    "        fixed_kernel_size,\n",
    "        num_classes,\n",
    "        linear_layer_features,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "    ):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        self.planes = 24\n",
    "        self.parallel_conv = nn.ModuleList()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        for i, kernel_size in enumerate(list(self.kernels)):\n",
    "            sep_conv = nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=self.planes,\n",
    "                kernel_size=(kernel_size),\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                dilation=dilation,\n",
    "                groups=groups,\n",
    "                bias=False,\n",
    "            )\n",
    "            self.parallel_conv.append(sep_conv)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        # self.relu = nn.ReLU(inplace=False)\n",
    "        # self.relu_1 = nn.ReLU()\n",
    "        # self.relu_2 = nn.ReLU()\n",
    "        self.relu_1 = nn.SiLU()\n",
    "        self.relu_2 = nn.SiLU()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=self.planes,\n",
    "            out_channels=self.planes,\n",
    "            kernel_size=fixed_kernel_size,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.block = self._make_resnet_layer(\n",
    "            kernel_size=fixed_kernel_size,\n",
    "            stride=1,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            padding=fixed_kernel_size // 2,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=6, stride=6, padding=2)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.in_channels,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            # dropout=0.2,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(in_features=linear_layer_features, out_features=num_classes)\n",
    "\n",
    "    def _make_resnet_layer(\n",
    "        self,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        blocks=9,\n",
    "        padding=0,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        base_width = self.planes\n",
    "\n",
    "        for i in range(blocks):\n",
    "            downsampling = nn.Sequential(\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "            )\n",
    "            layers.append(\n",
    "                ResNet_1D_Block(\n",
    "                    in_channels=self.planes,\n",
    "                    out_channels=self.planes,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    downsampling=downsampling,\n",
    "                    dilation=dilation,\n",
    "                    groups=groups,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "            )\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        out_sep = []\n",
    "        for i in range(len(self.kernels)):\n",
    "            sep = self.parallel_conv[i](x)\n",
    "            out_sep.append(sep)\n",
    "\n",
    "        out = torch.cat(out_sep, dim=2)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.block(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.avgpool(out)\n",
    "\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        rnn_out, _ = self.rnn(x.permute(0, 2, 1))\n",
    "        new_rnn_h = rnn_out[:, -1, :]\n",
    "\n",
    "        new_out = torch.cat([out, new_rnn_h], dim=1)\n",
    "        return new_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_out = self.extract_features(x)\n",
    "        result = self.fc(new_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab22963",
   "metadata": {
    "papermill": {
     "duration": 0.0155,
     "end_time": "2024-04-04T13:19:13.756915",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.741415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Adan Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fca09bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.789860Z",
     "iopub.status.busy": "2024-04-04T13:19:13.789515Z",
     "iopub.status.idle": "2024-04-04T13:19:13.815337Z",
     "shell.execute_reply": "2024-04-04T13:19:13.814443Z"
    },
    "papermill": {
     "duration": 0.044949,
     "end_time": "2024-04-04T13:19:13.817288",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.772339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Adan(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements a pytorch variant of Adan\n",
    "    Adan was proposed in\n",
    "    Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n",
    "    https://arxiv.org/abs/2208.06677\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n",
    "        lr (float, optional): learning rate. (default: 1e-3)\n",
    "        betas (Tuple[float, float, flot], optional): coefficients used for computing\n",
    "            running averages of gradient and its norm. (default: (0.98, 0.92, 0.99))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability. (default: 1e-8)\n",
    "        weight_decay (float, optional): decoupled weight decay (L2 penalty) (default: 0)\n",
    "        max_grad_norm (float, optional): value used to clip\n",
    "            global grad norm (default: 0.0 no clip)\n",
    "        no_prox (bool): how to perform the decoupled weight decay (default: False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        betas=(0.98, 0.92, 0.99),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.2,\n",
    "        max_grad_norm=0.0,\n",
    "        no_prox=False,\n",
    "    ):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError(\"Invalid Max grad norm: {}\".format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 2: {}\".format(betas[2]))\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            no_prox=no_prox,\n",
    "        )\n",
    "        super(Adan, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"no_prox\", False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group[\"step\"] = 0\n",
    "            for p in group[\"params\"]:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state[\"exp_avg_diff\"] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        if self.defaults[\"max_grad_norm\"] > 0:\n",
    "            device = self.param_groups[0][\"params\"][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults[\"max_grad_norm\"], device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group[\"params\"]:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group[\"eps\"]), max=1.0\n",
    "            )\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2, beta3 = group[\"betas\"]\n",
    "            # assume same step across group now to simplify things\n",
    "            # per parameter step can be easily support by making it tensor, or pass list into kernel\n",
    "            if \"step\" in group:\n",
    "                group[\"step\"] += 1\n",
    "            else:\n",
    "                group[\"step\"] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1 ** group[\"step\"]\n",
    "            bias_correction2 = 1.0 - beta2 ** group[\"step\"]\n",
    "            bias_correction3 = 1.0 - beta3 ** group[\"step\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_diff\"] = torch.zeros_like(p)\n",
    "\n",
    "                grad = p.grad.mul_(clip_global_grad_norm)\n",
    "                if \"pre_grad\" not in state or group[\"step\"] == 1:\n",
    "                    state[\"pre_grad\"] = grad\n",
    "\n",
    "                copy_grad = grad.clone()\n",
    "\n",
    "                exp_avg, exp_avg_sq, exp_avg_diff = (\n",
    "                    state[\"exp_avg\"],\n",
    "                    state[\"exp_avg_sq\"],\n",
    "                    state[\"exp_avg_diff\"],\n",
    "                )\n",
    "                diff = grad - state[\"pre_grad\"]\n",
    "\n",
    "                update = grad + beta2 * diff\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "                exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  # diff_t\n",
    "                exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  # n_t\n",
    "\n",
    "                denom = ((exp_avg_sq).sqrt() / math.sqrt(bias_correction3)).add_(\n",
    "                    group[\"eps\"]\n",
    "                )\n",
    "                update = (\n",
    "                    (\n",
    "                        exp_avg / bias_correction1\n",
    "                        + beta2 * exp_avg_diff / bias_correction2\n",
    "                    )\n",
    "                ).div_(denom)\n",
    "\n",
    "                if group[\"no_prox\"]:\n",
    "                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "                else:\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "                    p.data.div_(1 + group[\"lr\"] * group[\"weight_decay\"])\n",
    "\n",
    "                state[\"pre_grad\"] = copy_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2a390",
   "metadata": {
    "papermill": {
     "duration": 0.014458,
     "end_time": "2024-04-04T13:19:13.846264",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.831806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95388f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.876467Z",
     "iopub.status.busy": "2024-04-04T13:19:13.876197Z",
     "iopub.status.idle": "2024-04-04T13:19:13.887462Z",
     "shell.execute_reply": "2024-04-04T13:19:13.886593Z"
    },
    "papermill": {
     "duration": 0.028668,
     "end_time": "2024-04-04T13:19:13.889432",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.860764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    stage, fold, train_loader, model, criterion, optimizer, epoch, scheduler, device\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        eegs = batch[\"eeg\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(eegs)\n",
    "            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "\n",
    "        if CFG.log_show and (\n",
    "            step % CFG.log_step == 0 or step == (len(train_loader) - 1)\n",
    "        ):\n",
    "            # remain=timeSince(start, float(step + 1) / len(train_loader))\n",
    "            LOGGER.info(\n",
    "                f\"Epoch {epoch+1} [{step}/{len(train_loader)}] Loss: {losses.val:.4f} Loss Avg:{losses.avg:.4f}\"\n",
    "            )\n",
    "            # \"Elapsed {remain:s} Grad: {grad_norm:.4f}  LR: {cheduler.get_lr()[0]:.8f}\"\n",
    "\n",
    "        if CFG.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] loss\": losses.val,\n",
    "                    f\"[fold{fold}] lr\": scheduler.get_lr()[0],\n",
    "                }\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449206d",
   "metadata": {
    "papermill": {
     "duration": 0.014033,
     "end_time": "2024-04-04T13:19:13.917552",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.903519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Valid Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a71134ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:13.947418Z",
     "iopub.status.busy": "2024-04-04T13:19:13.947120Z",
     "iopub.status.idle": "2024-04-04T13:19:13.956798Z",
     "shell.execute_reply": "2024-04-04T13:19:13.955857Z"
    },
    "papermill": {
     "duration": 0.026993,
     "end_time": "2024-04-04T13:19:13.958599",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.931606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(stage, epoch, valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    start = end = time.time()\n",
    "\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        eegs = batch[\"eeg\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(eegs)\n",
    "            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(nn.Softmax(dim=1)(y_preds).to(\"cpu\").numpy())\n",
    "        targets.append(labels.to(\"cpu\").numpy())\n",
    "        end = time.time()\n",
    "\n",
    "        if CFG.log_show and (\n",
    "            step % CFG.log_step == 0 or step == (len(valid_loader) - 1)\n",
    "        ):\n",
    "            # remain=timeSince(start, float(step + 1) / len(valid_loader))\n",
    "            LOGGER.info(\n",
    "                f\"Epoch {epoch+1} VALIDATION: [{step}/{len(valid_loader)}] Val Loss: {losses.val:.4f} Val Loss Avg: {losses.avg:.4f}\"\n",
    "            )\n",
    "            # Elapsed {remain:s}\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7192e",
   "metadata": {
    "papermill": {
     "duration": 0.014052,
     "end_time": "2024-04-04T13:19:13.987158",
     "exception": false,
     "start_time": "2024-04-04T13:19:13.973106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db82f842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:14.016174Z",
     "iopub.status.busy": "2024-04-04T13:19:14.015881Z",
     "iopub.status.idle": "2024-04-04T13:19:14.025072Z",
     "shell.execute_reply": "2024-04-04T13:19:14.024256Z"
    },
    "papermill": {
     "duration": 0.025847,
     "end_time": "2024-04-04T13:19:14.027010",
     "exception": false,
     "start_time": "2024-04-04T13:19:14.001163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_optimizer(cfg, model, device, epochs, num_batches_per_epoch):\n",
    "    lr = cfg.lr\n",
    "    # lr = default_configs[\"lr\"]\n",
    "    if cfg.optimizer == \"SAM\":\n",
    "        base_optimizer = (\n",
    "            torch.optim.SGD\n",
    "        )  # define an optimizer for the \"sharpness-aware\" update\n",
    "        optimizer_model = SAM(\n",
    "            model.parameters(),\n",
    "            base_optimizer,\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            adaptive=True,\n",
    "        )\n",
    "    elif cfg.optimizer == \"Ranger21\":\n",
    "        optimizer_model = Ranger21(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            num_epochs=epochs,\n",
    "            num_batches_per_epoch=num_batches_per_epoch,\n",
    "        )\n",
    "    elif cfg.optimizer == \"SGD\":\n",
    "        optimizer_model = torch.optim.SGD(\n",
    "            model.parameters(), lr=lr, weight_decay=cfg.weight_decay, momentum=0.9\n",
    "        )\n",
    "    elif cfg.optimizer == \"Adam\":\n",
    "        optimizer_model = Adam(model.parameters(), lr=lr, weight_decay=CFG.weight_decay)\n",
    "    elif cfg.optimizer == \"AdamW\":\n",
    "        optimizer_model = AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=CFG.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == \"Lion\":\n",
    "        optimizer_model = Lion(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == \"Adan\":\n",
    "        optimizer_model = Adan(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    return optimizer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3b5f5",
   "metadata": {
    "papermill": {
     "duration": 0.014097,
     "end_time": "2024-04-04T13:19:14.055375",
     "exception": false,
     "start_time": "2024-04-04T13:19:14.041278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01cd6f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:14.084583Z",
     "iopub.status.busy": "2024-04-04T13:19:14.084311Z",
     "iopub.status.idle": "2024-04-04T13:19:14.090462Z",
     "shell.execute_reply": "2024-04-04T13:19:14.089503Z"
    },
    "papermill": {
     "duration": 0.023094,
     "end_time": "2024-04-04T13:19:14.092368",
     "exception": false,
     "start_time": "2024-04-04T13:19:14.069274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, epochs, steps_per_epoch):\n",
    "    if CFG.scheduler == \"ReduceLROnPlateau\":\n",
    "        scheduler = ReduceLROnPlateau(optimizer, **CFG.reduce_params)\n",
    "    elif CFG.scheduler == \"CosineAnnealingLR\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, **CFG.cosanneal_params)\n",
    "    elif CFG.scheduler == \"CosineAnnealingWarmRestarts\":\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, **CFG.cosanneal_res_params)\n",
    "    elif CFG.scheduler == \"OneCycleLR\":\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            epochs=epochs,\n",
    "            pct_start=0.0,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            max_lr=CFG.lr,\n",
    "            div_factor=25,\n",
    "            final_div_factor=4.0e-01,\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabae492",
   "metadata": {
    "papermill": {
     "duration": 0.014071,
     "end_time": "2024-04-04T13:19:14.120692",
     "exception": false,
     "start_time": "2024-04-04T13:19:14.106621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c75740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:14.150410Z",
     "iopub.status.busy": "2024-04-04T13:19:14.150137Z",
     "iopub.status.idle": "2024-04-04T13:19:14.170584Z",
     "shell.execute_reply": "2024-04-04T13:19:14.169728Z"
    },
    "papermill": {
     "duration": 0.037587,
     "end_time": "2024-04-04T13:19:14.172472",
     "exception": false,
     "start_time": "2024-04-04T13:19:14.134885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(stage, epochs, folds, fold, directory, prev_dir, eggs):\n",
    "    train_folds = folds[folds[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds[\"fold\"] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "\n",
    "    train_dataset = EEGDataset(\n",
    "        train_folds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        mode=\"train\",\n",
    "        eegs=eggs,\n",
    "        bandpass_filter=CFG.bandpass_filter,\n",
    "        rand_filter=CFG.rand_filter,\n",
    "    )\n",
    "        \n",
    "    valid_dataset = EEGDataset(\n",
    "        valid_folds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        mode=\"valid\",\n",
    "        eegs=eggs,\n",
    "        bandpass_filter=CFG.bandpass_filter,\n",
    "        #rand_filter=CFG.rand_filter,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CFG.batch_size * CFG.batch_koef_valid,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    LOGGER.info(\n",
    "        f\"========== stage: {stage} fold: {fold} training {len(train_loader)} / {len(valid_loader)} ==========\"\n",
    "    )\n",
    "\n",
    "    model = EEGNet(\n",
    "        kernels=CFG.kernels,\n",
    "        in_channels=CFG.in_channels,\n",
    "        fixed_kernel_size=CFG.fixed_kernel_size,\n",
    "        num_classes=CFG.target_size,\n",
    "        linear_layer_features=CFG.linear_layer_features,\n",
    "    )\n",
    "\n",
    "    if stage > 1:\n",
    "        model_weight = f\"{prev_dir}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage-1}_fold-{fold}_best.pth\"\n",
    "        checkpoint = torch.load(model_weight, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # CPMP: wrap the model to use all GPUs\n",
    "    if CFG.parallel:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = build_optimizer(\n",
    "        CFG, model, device, epochs=epochs, num_batches_per_epoch=len(train_loader)\n",
    "    )\n",
    "    scheduler = get_scheduler(\n",
    "        optimizer, epochs=epochs, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    best_score = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(\n",
    "            stage,\n",
    "            fold,\n",
    "            train_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        # eval\n",
    "        valid_dataset.set_offset(CFG.sample_offset)\n",
    "        avg_val_loss, predictions = valid_fn(\n",
    "            stage,\n",
    "            epoch,\n",
    "            valid_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "        \n",
    "        avg_loss_line = ''\n",
    "        if CFG.multi_validation:\n",
    "            multi_avg_val_loss = np.zeros(CFG.n_split_samples)\n",
    "            start = (2 * CFG.sample_delta) // CFG.n_split_samples\n",
    "            finish = (3 * CFG.sample_delta) // CFG.n_split_samples\n",
    "            delta = (finish - start) // 5\n",
    "            for i in range(CFG.n_split_samples):\n",
    "                valid_dataset.set_offset(start)\n",
    "                multi_avg_val_loss[i], _ = valid_fn(\n",
    "                    stage,\n",
    "                    epoch,\n",
    "                    valid_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    device,\n",
    "                )\n",
    "                avg_loss_line += f\" {multi_avg_val_loss[i]:.4f}\"\n",
    "                start += delta\n",
    "            avg_loss_line += f\" mean={np.mean(multi_avg_val_loss):.4f}\"\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} Avg Train Loss: {avg_loss:.4f} Avg Valid Loss: {avg_val_loss:.4f} / {avg_loss_line}\"\n",
    "        )\n",
    "        #   time: {elapsed:.0f}s\n",
    "        if CFG.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] stage\": stage,\n",
    "                    f\"[fold{fold}] epoch\": epoch + 1,\n",
    "                    f\"[fold{fold}] avg_train_loss\": avg_loss,\n",
    "                    f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                    #f\"[fold{fold}] score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if CFG.save_all_models:\n",
    "            torch.save(\n",
    "                {\"model\": model.module.state_dict(), \"predictions\": predictions},\n",
    "                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_epoch-{epoch}_val-{avg_val_loss:.4f}_train-{avg_loss:.4f}.pth\",\n",
    "            )\n",
    "\n",
    "        if best_score > avg_val_loss:\n",
    "            best_score = avg_val_loss\n",
    "            LOGGER.info(f\"Epoch {epoch+1} Save Best Valid Loss: {avg_val_loss:.4f}\")\n",
    "            # CPMP: save the original model. It is stored as the module attribute of the DP model.\n",
    "            torch.save(\n",
    "                {\"model\": model.module.state_dict(), \"predictions\": predictions},\n",
    "                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "\n",
    "    # valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "    valid_folds[CFG.pred_cols] = predictions\n",
    "    valid_folds[CFG.target_cols] = valid_labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return valid_folds, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b822ce",
   "metadata": {
    "papermill": {
     "duration": 0.014008,
     "end_time": "2024-04-04T13:19:14.200359",
     "exception": false,
     "start_time": "2024-04-04T13:19:14.186351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b129d5db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:14.229636Z",
     "iopub.status.busy": "2024-04-04T13:19:14.229336Z",
     "iopub.status.idle": "2024-04-04T13:19:14.709989Z",
     "shell.execute_reply": "2024-04-04T13:19:14.709011Z"
    },
    "papermill": {
     "duration": 0.497646,
     "end_time": "2024-04-04T13:19:14.712153",
     "exception": false,
     "start_time": "2024-04-04T13:19:14.214507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (106800, 15)\n",
      "Targets ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
      "There are 1950 patients in the training data.\n",
      "There are 17089 EEG IDs in the training data.\n",
      "There are 20183 unique eeg_id + votes in the training data.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.file_train)\n",
    "TARGETS = train.columns[-6:]\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Targets\", list(TARGETS))\n",
    "\n",
    "train[\"total_evaluators\"] = train[CFG.target_cols].sum(axis=1)\n",
    "\n",
    "train_uniq = train.drop_duplicates(subset=[\"eeg_id\"] + list(TARGETS))\n",
    "\n",
    "print(f\"There are {train.patient_id.nunique()} patients in the training data.\")\n",
    "print(f\"There are {train.eeg_id.nunique()} EEG IDs in the training data.\")\n",
    "print(f\"There are {train_uniq.shape[0]} unique eeg_id + votes in the training data.\")\n",
    "\n",
    "if CFG.visualize:\n",
    "    train_uniq.eeg_id.value_counts().value_counts().plot(\n",
    "        kind=\"bar\",\n",
    "        title=f\"Distribution of Count of EEG w Unique Vote: \"\n",
    "        f\"{train_uniq.shape[0]} examples\",\n",
    "    )\n",
    "\n",
    "del train_uniq\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4b0d5e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:14.742452Z",
     "iopub.status.busy": "2024-04-04T13:19:14.742145Z",
     "iopub.status.idle": "2024-04-04T13:19:15.090109Z",
     "shell.execute_reply": "2024-04-04T13:19:15.089195Z"
    },
    "papermill": {
     "duration": 0.365571,
     "end_time": "2024-04-04T13:19:15.092230",
     "exception": false,
     "start_time": "2024-04-04T13:19:14.726659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 raw eeg features\n",
      "['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n"
     ]
    }
   ],
   "source": [
    "if CFG.visualize:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n",
    "    plt.title(\"Histogram of Total Evaluators\")\n",
    "    plt.xlabel(\"Total Evaluators\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "tst_eeg_df = pd.read_parquet(CFG.file_features_test)\n",
    "tst_eeg_features = tst_eeg_df.columns\n",
    "print(f\"There are {len(tst_eeg_features)} raw eeg features\")\n",
    "print(list(tst_eeg_features))\n",
    "del tst_eeg_df\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff05f22",
   "metadata": {
    "papermill": {
     "duration": 0.014023,
     "end_time": "2024-04-04T13:19:15.120906",
     "exception": false,
     "start_time": "2024-04-04T13:19:15.106883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d9e51bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:19:15.150333Z",
     "iopub.status.busy": "2024-04-04T13:19:15.150064Z",
     "iopub.status.idle": "2024-04-04T13:21:01.204365Z",
     "shell.execute_reply": "2024-04-04T13:21:01.203417Z"
    },
    "papermill": {
     "duration": 106.087086,
     "end_time": "2024-04-04T13:21:01.222218",
     "exception": false,
     "start_time": "2024-04-04T13:19:15.135132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20183\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPD</th>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPD</th>\n",
       "      <th>4</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seizure</th>\n",
       "      <th>4</th>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   eeg_id  eeg_sub_id  \\\n",
       "expert_consensus total_evaluators                       \n",
       "GPD              4                     96          96   \n",
       "GRDA             4                     10          10   \n",
       "LPD              4                    250         250   \n",
       "LRDA             4                     32          32   \n",
       "Other            4                    172         172   \n",
       "Seizure          4                    192         192   \n",
       "\n",
       "                                   eeg_label_offset_seconds  spectrogram_id  \\\n",
       "expert_consensus total_evaluators                                             \n",
       "GPD              4                                       96              96   \n",
       "GRDA             4                                       10              10   \n",
       "LPD              4                                      250             250   \n",
       "LRDA             4                                       32              32   \n",
       "Other            4                                      172             172   \n",
       "Seizure          4                                      192             192   \n",
       "\n",
       "                                   spectrogram_sub_id  \\\n",
       "expert_consensus total_evaluators                       \n",
       "GPD              4                                 96   \n",
       "GRDA             4                                 10   \n",
       "LPD              4                                250   \n",
       "LRDA             4                                 32   \n",
       "Other            4                                172   \n",
       "Seizure          4                                192   \n",
       "\n",
       "                                   spectrogram_label_offset_seconds  label_id  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GPD              4                                               96        96   \n",
       "GRDA             4                                               10        10   \n",
       "LPD              4                                              250       250   \n",
       "LRDA             4                                               32        32   \n",
       "Other            4                                              172       172   \n",
       "Seizure          4                                              192       192   \n",
       "\n",
       "                                   patient_id  seizure_vote  lpd_vote  \\\n",
       "expert_consensus total_evaluators                                       \n",
       "GPD              4                         96            96        96   \n",
       "GRDA             4                         10            10        10   \n",
       "LPD              4                        250           250       250   \n",
       "LRDA             4                         32            32        32   \n",
       "Other            4                        172           172       172   \n",
       "Seizure          4                        192           192       192   \n",
       "\n",
       "                                   gpd_vote  lrda_vote  grda_vote  other_vote  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GPD              4                       96         96         96          96   \n",
       "GRDA             4                       10         10         10          10   \n",
       "LPD              4                      250        250        250         250   \n",
       "LRDA             4                       32         32         32          32   \n",
       "Other            4                      172        172        172         172   \n",
       "Seizure          4                      192        192        192         192   \n",
       "\n",
       "                                   target  \n",
       "expert_consensus total_evaluators          \n",
       "GPD              4                     96  \n",
       "GRDA             4                     10  \n",
       "LPD              4                    250  \n",
       "LRDA             4                     32  \n",
       "Other            4                    172  \n",
       "Seizure          4                    192  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "all_eeg_specs = np.load(CFG.file_eeg_specs, allow_pickle=True).item()\n",
    "\n",
    "train = train[train[\"label_id\"].isin(all_eeg_specs.keys())].copy()\n",
    "print(train.shape[0])\n",
    "\n",
    "y_data = train[TARGETS].values + 0.166666667  # Regularization value\n",
    "y_data = y_data / y_data.sum(axis=1, keepdims=True)\n",
    "train[TARGETS] = y_data\n",
    "\n",
    "train[\"target\"] = train[\"expert_consensus\"]\n",
    "\n",
    "train[train['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fb39545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:21:01.293801Z",
     "iopub.status.busy": "2024-04-04T13:21:01.293046Z",
     "iopub.status.idle": "2024-04-04T13:21:01.341608Z",
     "shell.execute_reply": "2024-04-04T13:21:01.340635Z"
    },
    "papermill": {
     "duration": 0.106552,
     "end_time": "2024-04-04T13:21:01.343618",
     "exception": false,
     "start_time": "2024-04-04T13:21:01.237066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12546\n",
      "6492\n"
     ]
    }
   ],
   "source": [
    "if CFG.test_total_eval > 0:\n",
    "    train['key_id'] = range(train.shape[0])\n",
    "\n",
    "    train_pop_olds = []\n",
    "    for total_eval in CFG.total_evals_old:\n",
    "        if type(total_eval) is list:\n",
    "            pop_idx = (train[\"total_evaluators\"] >= total_eval[0][0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[0][1]\n",
    "            ) | (train[\"total_evaluators\"] >= total_eval[1][0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[1][1]\n",
    "            )\n",
    "        else:\n",
    "            pop_idx = (train[\"total_evaluators\"] >= total_eval[0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[1]\n",
    "            )\n",
    "\n",
    "        train_pop = train[pop_idx].copy().reset_index()\n",
    "\n",
    "        sgkf = GroupKFold(n_splits=CFG.n_fold)\n",
    "        train_pop[\"fold\"] = -1\n",
    "        for fold_id, (_, val_idx) in enumerate(\n",
    "            sgkf.split(train_pop, y=train_pop[\"target\"], groups=train_pop[\"patient_id\"])\n",
    "        ):\n",
    "            train_pop.loc[val_idx, \"fold\"] = fold_id\n",
    "\n",
    "        train_pop_olds.append(train_pop)\n",
    "        print(train_pop.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dab6b31c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:21:01.376321Z",
     "iopub.status.busy": "2024-04-04T13:21:01.375743Z",
     "iopub.status.idle": "2024-04-04T13:21:01.442184Z",
     "shell.execute_reply": "2024-04-04T13:21:01.441300Z"
    },
    "papermill": {
     "duration": 0.0845,
     "end_time": "2024-04-04T13:21:01.444163",
     "exception": false,
     "start_time": "2024-04-04T13:21:01.359663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13595\n",
      "6492\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "      <th>key_id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPD</th>\n",
       "      <th>4</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seizure</th>\n",
       "      <th>4</th>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   index  eeg_id  eeg_sub_id  \\\n",
       "expert_consensus total_evaluators                              \n",
       "GRDA             4                    10      10          10   \n",
       "LPD              4                   250     250         250   \n",
       "LRDA             4                    32      32          32   \n",
       "Other            4                   172     172         172   \n",
       "Seizure          4                   192     192         192   \n",
       "\n",
       "                                   eeg_label_offset_seconds  spectrogram_id  \\\n",
       "expert_consensus total_evaluators                                             \n",
       "GRDA             4                                       10              10   \n",
       "LPD              4                                      250             250   \n",
       "LRDA             4                                       32              32   \n",
       "Other            4                                      172             172   \n",
       "Seizure          4                                      192             192   \n",
       "\n",
       "                                   spectrogram_sub_id  \\\n",
       "expert_consensus total_evaluators                       \n",
       "GRDA             4                                 10   \n",
       "LPD              4                                250   \n",
       "LRDA             4                                 32   \n",
       "Other            4                                172   \n",
       "Seizure          4                                192   \n",
       "\n",
       "                                   spectrogram_label_offset_seconds  label_id  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GRDA             4                                               10        10   \n",
       "LPD              4                                              250       250   \n",
       "LRDA             4                                               32        32   \n",
       "Other            4                                              172       172   \n",
       "Seizure          4                                              192       192   \n",
       "\n",
       "                                   patient_id  seizure_vote  lpd_vote  \\\n",
       "expert_consensus total_evaluators                                       \n",
       "GRDA             4                         10            10        10   \n",
       "LPD              4                        250           250       250   \n",
       "LRDA             4                         32            32        32   \n",
       "Other            4                        172           172       172   \n",
       "Seizure          4                        192           192       192   \n",
       "\n",
       "                                   gpd_vote  lrda_vote  grda_vote  other_vote  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GRDA             4                       10         10         10          10   \n",
       "LPD              4                      250        250        250         250   \n",
       "LRDA             4                       32         32         32          32   \n",
       "Other            4                      172        172        172         172   \n",
       "Seizure          4                      192        192        192         192   \n",
       "\n",
       "                                   target  key_id  fold  \n",
       "expert_consensus total_evaluators                        \n",
       "GRDA             4                     10      10    10  \n",
       "LPD              4                    250     250   250  \n",
       "LRDA             4                     32      32    32  \n",
       "Other            4                    172     172   172  \n",
       "Seizure          4                    192     192   192  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pops = []\n",
    "for eval_list in CFG.total_evaluators:\n",
    "    result=[]\n",
    "    train_pop = train  \n",
    "    for eval_dict in eval_list:\n",
    "        band = eval_dict['band']\n",
    "        pop_idx = (train_pop[\"total_evaluators\"] >= band[0]) \n",
    "        pop_idx &= (train_pop[\"total_evaluators\"] <= band[1])\n",
    "        for exclude in eval_dict['excl_evals']:\n",
    "            pop_idx &= ~(train_pop['expert_consensus'] == exclude)\n",
    "            pass\n",
    "        result.append(train_pop[pop_idx])\n",
    "    train_pop = pd.concat(result).copy().reset_index()\n",
    "\n",
    "    sgkf = GroupKFold(n_splits=CFG.n_fold)\n",
    "    train_pop[\"fold\"] = -1\n",
    "    for fold_id, (_, val_idx) in enumerate(\n",
    "        sgkf.split(train_pop, y=train_pop[\"target\"], groups=train_pop[\"patient_id\"])\n",
    "    ):\n",
    "        train_pop.loc[val_idx, \"fold\"] = fold_id\n",
    "\n",
    "    train_pops.append(train_pop)\n",
    "    print(train_pop.shape[0])\n",
    "\n",
    "train_0 = train_pops[0]\n",
    "train_0[train_0['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7aa38ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:21:01.476711Z",
     "iopub.status.busy": "2024-04-04T13:21:01.476183Z",
     "iopub.status.idle": "2024-04-04T13:21:01.735977Z",
     "shell.execute_reply": "2024-04-04T13:21:01.735092Z"
    },
    "papermill": {
     "duration": 0.281128,
     "end_time": "2024-04-04T13:21:01.740955",
     "exception": false,
     "start_time": "2024-04-04T13:21:01.459827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th>target</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>1626798710</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1219001</td>\n",
       "      <td>2</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3631726128</td>\n",
       "      <td>23435</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>2529955608</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1219001</td>\n",
       "      <td>4</td>\n",
       "      <td>190.0</td>\n",
       "      <td>4265493714</td>\n",
       "      <td>23435</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>62</td>\n",
       "      <td>989810287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2843061</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116172961</td>\n",
       "      <td>13521</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>5</td>\n",
       "      <td>Other</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>172</td>\n",
       "      <td>4000022002</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7122706</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462363590</td>\n",
       "      <td>11471</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>177</td>\n",
       "      <td>3429523414</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7236473</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1420869020</td>\n",
       "      <td>24909</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13661</th>\n",
       "      <td>106554</td>\n",
       "      <td>2293242511</td>\n",
       "      <td>21</td>\n",
       "      <td>188.0</td>\n",
       "      <td>2141866254</td>\n",
       "      <td>21</td>\n",
       "      <td>188.0</td>\n",
       "      <td>173246725</td>\n",
       "      <td>53838</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13673</th>\n",
       "      <td>106632</td>\n",
       "      <td>4288875638</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2145358771</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3377992611</td>\n",
       "      <td>200</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13675</th>\n",
       "      <td>106653</td>\n",
       "      <td>728496959</td>\n",
       "      <td>17</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2145358771</td>\n",
       "      <td>21</td>\n",
       "      <td>434.0</td>\n",
       "      <td>2948298992</td>\n",
       "      <td>200</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13676</th>\n",
       "      <td>106662</td>\n",
       "      <td>728496959</td>\n",
       "      <td>26</td>\n",
       "      <td>306.0</td>\n",
       "      <td>2145358771</td>\n",
       "      <td>30</td>\n",
       "      <td>586.0</td>\n",
       "      <td>1610933349</td>\n",
       "      <td>200</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13686</th>\n",
       "      <td>106770</td>\n",
       "      <td>3349371726</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2146170054</td>\n",
       "      <td>7</td>\n",
       "      <td>124.0</td>\n",
       "      <td>142522487</td>\n",
       "      <td>33380</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1241 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index      eeg_id  eeg_sub_id  eeg_label_offset_seconds  \\\n",
       "3          30  1626798710           0                       0.0   \n",
       "4          32  2529955608           0                       0.0   \n",
       "11         62   989810287           0                       0.0   \n",
       "31        172  4000022002           0                       0.0   \n",
       "33        177  3429523414           3                      10.0   \n",
       "...       ...         ...         ...                       ...   \n",
       "13661  106554  2293242511          21                     188.0   \n",
       "13673  106632  4288875638           0                       0.0   \n",
       "13675  106653   728496959          17                     154.0   \n",
       "13676  106662   728496959          26                     306.0   \n",
       "13686  106770  3349371726           0                       0.0   \n",
       "\n",
       "       spectrogram_id  spectrogram_sub_id  spectrogram_label_offset_seconds  \\\n",
       "3             1219001                   2                              74.0   \n",
       "4             1219001                   4                             190.0   \n",
       "11            2843061                   0                               0.0   \n",
       "31            7122706                   0                               0.0   \n",
       "33            7236473                   3                              10.0   \n",
       "...               ...                 ...                               ...   \n",
       "13661      2141866254                  21                             188.0   \n",
       "13673      2145358771                   0                               0.0   \n",
       "13675      2145358771                  21                             434.0   \n",
       "13676      2145358771                  30                             586.0   \n",
       "13686      2146170054                   7                             124.0   \n",
       "\n",
       "         label_id  patient_id expert_consensus  seizure_vote  lpd_vote  \\\n",
       "3      3631726128       23435          Seizure      0.527778  0.027778   \n",
       "4      4265493714       23435          Seizure      0.527778  0.027778   \n",
       "11      116172961       13521            Other      0.027778  0.194444   \n",
       "31      462363590       11471          Seizure      0.527778  0.027778   \n",
       "33     1420869020       24909          Seizure      0.694444  0.027778   \n",
       "...           ...         ...              ...           ...       ...   \n",
       "13661   173246725       53838          Seizure      0.527778  0.194444   \n",
       "13673  3377992611         200          Seizure      0.694444  0.194444   \n",
       "13675  2948298992         200          Seizure      0.694444  0.194444   \n",
       "13676  1610933349         200          Seizure      0.861111  0.027778   \n",
       "13686   142522487       33380          Seizure      0.527778  0.027778   \n",
       "\n",
       "       gpd_vote  lrda_vote  grda_vote  other_vote  total_evaluators   target  \\\n",
       "3      0.361111   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "4      0.361111   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "11     0.027778   0.027778   0.027778    0.694444                 5    Other   \n",
       "31     0.027778   0.027778   0.027778    0.361111                 5  Seizure   \n",
       "33     0.027778   0.027778   0.027778    0.194444                 5  Seizure   \n",
       "...         ...        ...        ...         ...               ...      ...   \n",
       "13661  0.194444   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "13673  0.027778   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "13675  0.027778   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "13676  0.027778   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "13686  0.361111   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "\n",
       "            Exist  \n",
       "3      right_only  \n",
       "4      right_only  \n",
       "11     right_only  \n",
       "31     right_only  \n",
       "33     right_only  \n",
       "...           ...  \n",
       "13661  right_only  \n",
       "13673  right_only  \n",
       "13675  right_only  \n",
       "13676  right_only  \n",
       "13686  right_only  \n",
       "\n",
       "[1241 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CFG.test_total_eval > 0:\n",
    "    df_old = train_pop_olds[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n",
    "    df_new = train_pops[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n",
    "\n",
    "    #outer merge the two DataFrames, adding an indicator column called 'Exist'\n",
    "    diff_df = pd.merge(df_old, df_new, how='outer', indicator='Exist')\n",
    "\n",
    "    #find which rows don't exist in both DataFrames\n",
    "    diff_df = diff_df.loc[diff_df['Exist'] != 'both']\n",
    "    display(diff_df)\n",
    "\n",
    "    del df_old, df_new, diff_df, train_pop_olds\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c7b4270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:21:01.775526Z",
     "iopub.status.busy": "2024-04-04T13:21:01.774938Z",
     "iopub.status.idle": "2024-04-04T13:21:01.909759Z",
     "shell.execute_reply": "2024-04-04T13:21:01.908846Z"
    },
    "papermill": {
     "duration": 0.154157,
     "end_time": "2024-04-04T13:21:01.911684",
     "exception": false,
     "start_time": "2024-04-04T13:21:01.757527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.visualize:\n",
    "    print(\"Pop 1: train unique eeg_id + votes shape:\", train_pops[0].shape)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n",
    "    plt.title(\"Histogram of Total Evaluators\")\n",
    "    plt.xlabel(\"Total Evaluators\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "del all_eeg_specs\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db7745",
   "metadata": {
    "papermill": {
     "duration": 0.016119,
     "end_time": "2024-04-04T13:21:01.944486",
     "exception": false,
     "start_time": "2024-04-04T13:21:01.928367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deduplicate Train EEG Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "522b2469",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:21:01.978593Z",
     "iopub.status.busy": "2024-04-04T13:21:01.977863Z",
     "iopub.status.idle": "2024-04-04T13:23:07.699478Z",
     "shell.execute_reply": "2024-04-04T13:23:07.698644Z"
    },
    "papermill": {
     "duration": 125.741275,
     "end_time": "2024-04-04T13:23:07.701902",
     "exception": false,
     "start_time": "2024-04-04T13:21:01.960627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "if CFG.create_eegs:\n",
    "    all_eegs = {}\n",
    "    visualize = 1 if CFG.visualize else 0\n",
    "    eeg_ids = train.eeg_id.unique()\n",
    "\n",
    "    for i, eeg_id in tqdm(enumerate(eeg_ids)):\n",
    "\n",
    "        # Сохранить ЭЭГ в словаре Python для массивов numpy\n",
    "        eeg_path = CFG.path_train / f\"{eeg_id}.parquet\"\n",
    "\n",
    "        # Вырезаем среднюю 50 секундную часть и заполняем по среднему Nan\n",
    "        data = eeg_from_parquet(eeg_path, display=i < visualize)\n",
    "        all_eegs[eeg_id] = data\n",
    "\n",
    "        if i == visualize:\n",
    "            if CFG.create_eegs:\n",
    "                print(\n",
    "                    f\"Processing {train['eeg_id'].nunique()} eeg parquets... \", end=\"\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Reading {len(eeg_ids)} eeg NumPys from disk.\")\n",
    "                break\n",
    "    np.save(\"./eegs\", all_eegs)\n",
    "\n",
    "else:\n",
    "    all_eegs = np.load(CFG.file_raw_eeg, allow_pickle=True).item()\n",
    "\n",
    "if CFG.visualize:\n",
    "    frequencies = [1, 2, 4, 8, 16][::-1]  # frequencies in Hz\n",
    "    x = [all_eegs[eeg_ids[0]][:, 0]]  # select one EEG feature\n",
    "\n",
    "    for frequency in frequencies:\n",
    "        x.append(butter_lowpass_filter(x[0], cutoff_freq=frequency))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(range(CFG.nsamples), x[0], label=\"without filter\")\n",
    "    for k in range(1, len(x)):\n",
    "        plt.plot(\n",
    "            range(CFG.nsamples),\n",
    "            x[k] - k * (x[0].max() - x[0].min()),\n",
    "            label=f\"with filter {frequencies[k-1]}Hz\",\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Butter Low-Pass Filter Examples\", size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19f8bb44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:23:07.737334Z",
     "iopub.status.busy": "2024-04-04T13:23:07.736565Z",
     "iopub.status.idle": "2024-04-04T13:23:07.747686Z",
     "shell.execute_reply": "2024-04-04T13:23:07.746858Z"
    },
    "papermill": {
     "duration": 0.030358,
     "end_time": "2024-04-04T13:23:07.749522",
     "exception": false,
     "start_time": "2024-04-04T13:23:07.719164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.visualize:\n",
    "    train_dataset = EEGDataset(\n",
    "        train_pops[0], batch_size=CFG.batch_size, eegs=all_eegs, mode=\"train\"\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    output = train_dataset[0]\n",
    "    X, y = output[\"eeg\"], output[\"labels\"]\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    iot = torch.randn(2, CFG.nsamples, CFG.in_channels)  # .cuda()\n",
    "    model = EEGNet(\n",
    "        kernels=CFG.kernels,\n",
    "        in_channels=CFG.in_channels,\n",
    "        fixed_kernel_size=CFG.fixed_kernel_size,\n",
    "        num_classes=CFG.target_size,\n",
    "        linear_layer_features=CFG.linear_layer_features,\n",
    "    )\n",
    "    output = model(iot)\n",
    "    print(output.shape)\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X = batch.pop(\"eeg\")\n",
    "        y = batch.pop(\"labels\")\n",
    "        for item in range(4):\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            offset = 0\n",
    "            for col in range(X.shape[-1]):\n",
    "                if col != 0:\n",
    "                    offset -= X[item, :, col].min()\n",
    "                plt.plot(\n",
    "                    range(CFG.nsamples),\n",
    "                    X[item, :, col] + offset,\n",
    "                    label=f\"feature {col+1}\",\n",
    "                )\n",
    "                offset += X[item, :, col].max()\n",
    "            tt = f\"{y[col][0]:0.1f}\"\n",
    "            for t in y[col][1:]:\n",
    "                tt += f\", {t:0.1f}\"\n",
    "            plt.title(f\"EEG_Id = {eeg_ids[item]}\\nTarget = {tt}\", size=14)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        break\n",
    "\n",
    "    del iot, model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213530d4",
   "metadata": {
    "papermill": {
     "duration": 0.015997,
     "end_time": "2024-04-04T13:23:07.781676",
     "exception": false,
     "start_time": "2024-04-04T13:23:07.765679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f419c2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:23:07.815433Z",
     "iopub.status.busy": "2024-04-04T13:23:07.815181Z",
     "iopub.status.idle": "2024-04-04T13:23:07.822485Z",
     "shell.execute_reply": "2024-04-04T13:23:07.821663Z"
    },
    "papermill": {
     "duration": 0.026606,
     "end_time": "2024-04-04T13:23:07.824347",
     "exception": false,
     "start_time": "2024-04-04T13:23:07.797741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(preds, targets):\n",
    "    oof = pd.DataFrame(preds.copy())\n",
    "    oof[\"id\"] = np.arange(len(oof))\n",
    "    true = pd.DataFrame(targets.copy())\n",
    "    true[\"id\"] = np.arange(len(true))\n",
    "    cv = kaggle_kl_div.score(solution=true, submission=oof, row_id_column_name=\"id\")\n",
    "    return cv\n",
    "\n",
    "\n",
    "def get_result(result_df):\n",
    "    gt = result_df[[\"eeg_id\"] + CFG.target_cols]\n",
    "    gt.sort_values(by=\"eeg_id\", inplace=True)\n",
    "    gt.reset_index(inplace=True, drop=True)\n",
    "    preds = result_df[[\"eeg_id\"] + CFG.pred_cols]\n",
    "    preds.columns = [\"eeg_id\"] + CFG.target_cols\n",
    "    preds.sort_values(by=\"eeg_id\", inplace=True)\n",
    "    preds.reset_index(inplace=True, drop=True)\n",
    "    score_loss = get_score(gt[CFG.target_cols], preds[CFG.target_cols])\n",
    "    LOGGER.info(f\"Score with best loss weights: {score_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24313f6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:23:07.857775Z",
     "iopub.status.busy": "2024-04-04T13:23:07.857534Z",
     "iopub.status.idle": "2024-04-04T13:23:07.866453Z",
     "shell.execute_reply": "2024-04-04T13:23:07.865638Z"
    },
    "papermill": {
     "duration": 0.027814,
     "end_time": "2024-04-04T13:23:07.868248",
     "exception": false,
     "start_time": "2024-04-04T13:23:07.840434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" and CFG.train_by_stages:\n",
    "    seed_torch(seed=CFG.seed)\n",
    "\n",
    "    prev_dir = \"\"\n",
    "    for stage in range(len(CFG.total_evaluators)):\n",
    "        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "        if not os.path.exists(pop_dir):\n",
    "            os.makedirs(pop_dir)\n",
    "\n",
    "        if stage not in CFG.train_stages:\n",
    "            prev_dir = pop_dir\n",
    "            continue\n",
    "\n",
    "        oof_df = pd.DataFrame()\n",
    "        scores = []\n",
    "        for fold in CFG.train_folds:\n",
    "            train_oof_df, score = train_loop(\n",
    "                stage=stage + 1,\n",
    "                epochs=CFG.epochs[stage],\n",
    "                fold=fold,\n",
    "                folds=train_pops[stage],\n",
    "                directory=pop_dir,\n",
    "                prev_dir=prev_dir,\n",
    "                eggs=all_eegs,\n",
    "            )\n",
    "\n",
    "            oof_df = pd.concat([oof_df, train_oof_df])\n",
    "            scores.append(score)\n",
    "\n",
    "            LOGGER.info(f\"========== stage: {stage+1} fold: {fold} result ==========\")\n",
    "            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n",
    "\n",
    "        LOGGER.info(f\"==================== CV ====================\")\n",
    "        LOGGER.info(f\"Score with best loss weights: {np.mean(scores):.4f}\")\n",
    "\n",
    "        oof_df.reset_index(drop=True, inplace=True)\n",
    "        oof_df.to_csv(\n",
    "            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        prev_dir = pop_dir\n",
    "\n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3481abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T13:23:07.901600Z",
     "iopub.status.busy": "2024-04-04T13:23:07.901355Z",
     "iopub.status.idle": "2024-04-04T18:03:43.513310Z",
     "shell.execute_reply": "2024-04-04T18:03:43.512103Z"
    },
    "papermill": {
     "duration": 16835.631274,
     "end_time": "2024-04-04T18:03:43.515704",
     "exception": false,
     "start_time": "2024-04-04T13:23:07.884430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== stage: 1 fold: 0 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5645 Avg Valid Loss: 0.5683 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5683\n",
      "Epoch 2 Avg Train Loss: 0.4784 Avg Valid Loss: 0.5122 / \n",
      "Epoch 2 Save Best Valid Loss: 0.5122\n",
      "Epoch 3 Avg Train Loss: 0.4448 Avg Valid Loss: 0.5144 / \n",
      "Epoch 4 Avg Train Loss: 0.4294 Avg Valid Loss: 0.4826 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4826\n",
      "Epoch 5 Avg Train Loss: 0.4163 Avg Valid Loss: 0.4961 / \n",
      "Epoch 6 Avg Train Loss: 0.4139 Avg Valid Loss: 0.4515 / \n",
      "Epoch 6 Save Best Valid Loss: 0.4515\n",
      "Epoch 7 Avg Train Loss: 0.4061 Avg Valid Loss: 0.4871 / \n",
      "Epoch 8 Avg Train Loss: 0.4008 Avg Valid Loss: 0.4722 / \n",
      "Epoch 9 Avg Train Loss: 0.3940 Avg Valid Loss: 0.4472 / \n",
      "Epoch 9 Save Best Valid Loss: 0.4472\n",
      "Epoch 10 Avg Train Loss: 0.3871 Avg Valid Loss: 0.4847 / \n",
      "Epoch 11 Avg Train Loss: 0.3900 Avg Valid Loss: 0.4656 / \n",
      "Epoch 12 Avg Train Loss: 0.3850 Avg Valid Loss: 0.4688 / \n",
      "Epoch 13 Avg Train Loss: 0.3816 Avg Valid Loss: 0.4753 / \n",
      "Epoch 14 Avg Train Loss: 0.3787 Avg Valid Loss: 0.4766 / \n",
      "Epoch 15 Avg Train Loss: 0.3746 Avg Valid Loss: 0.4605 / \n",
      "Epoch 16 Avg Train Loss: 0.3712 Avg Valid Loss: 0.4676 / \n",
      "Epoch 17 Avg Train Loss: 0.3741 Avg Valid Loss: 0.4647 / \n",
      "Epoch 18 Avg Train Loss: 0.3734 Avg Valid Loss: 0.4503 / \n",
      "Epoch 19 Avg Train Loss: 0.3706 Avg Valid Loss: 0.4612 / \n",
      "Epoch 20 Avg Train Loss: 0.3684 Avg Valid Loss: 0.4774 / \n",
      "Epoch 21 Avg Train Loss: 0.3663 Avg Valid Loss: 0.4752 / \n",
      "Epoch 22 Avg Train Loss: 0.3650 Avg Valid Loss: 0.4546 / \n",
      "Epoch 23 Avg Train Loss: 0.3635 Avg Valid Loss: 0.4711 / \n",
      "Epoch 24 Avg Train Loss: 0.3633 Avg Valid Loss: 0.4661 / \n",
      "Epoch 25 Avg Train Loss: 0.3599 Avg Valid Loss: 0.4490 / \n",
      "Epoch 26 Avg Train Loss: 0.3607 Avg Valid Loss: 0.4545 / \n",
      "Epoch 27 Avg Train Loss: 0.3615 Avg Valid Loss: 0.4404 / \n",
      "Epoch 27 Save Best Valid Loss: 0.4404\n",
      "Epoch 28 Avg Train Loss: 0.3576 Avg Valid Loss: 0.4461 / \n",
      "Epoch 29 Avg Train Loss: 0.3555 Avg Valid Loss: 0.4431 / \n",
      "Epoch 30 Avg Train Loss: 0.3538 Avg Valid Loss: 0.4565 / \n",
      "Epoch 31 Avg Train Loss: 0.3528 Avg Valid Loss: 0.4411 / \n",
      "Epoch 32 Avg Train Loss: 0.3553 Avg Valid Loss: 0.4625 / \n",
      "Epoch 33 Avg Train Loss: 0.3523 Avg Valid Loss: 0.4667 / \n",
      "Epoch 34 Avg Train Loss: 0.3506 Avg Valid Loss: 0.4512 / \n",
      "Epoch 35 Avg Train Loss: 0.3505 Avg Valid Loss: 0.4701 / \n",
      "Epoch 36 Avg Train Loss: 0.3558 Avg Valid Loss: 0.4622 / \n",
      "Epoch 37 Avg Train Loss: 0.3449 Avg Valid Loss: 0.4766 / \n",
      "Epoch 38 Avg Train Loss: 0.3538 Avg Valid Loss: 0.4536 / \n",
      "Epoch 39 Avg Train Loss: 0.3507 Avg Valid Loss: 0.4678 / \n",
      "Epoch 40 Avg Train Loss: 0.3472 Avg Valid Loss: 0.4322 / \n",
      "Epoch 40 Save Best Valid Loss: 0.4322\n",
      "Epoch 41 Avg Train Loss: 0.3442 Avg Valid Loss: 0.4447 / \n",
      "Epoch 42 Avg Train Loss: 0.3414 Avg Valid Loss: 0.4496 / \n",
      "Epoch 43 Avg Train Loss: 0.3457 Avg Valid Loss: 0.4763 / \n",
      "Epoch 44 Avg Train Loss: 0.3420 Avg Valid Loss: 0.4639 / \n",
      "Epoch 45 Avg Train Loss: 0.3455 Avg Valid Loss: 0.4809 / \n",
      "Epoch 46 Avg Train Loss: 0.3412 Avg Valid Loss: 0.4720 / \n",
      "Epoch 47 Avg Train Loss: 0.3423 Avg Valid Loss: 0.4622 / \n",
      "Epoch 48 Avg Train Loss: 0.3456 Avg Valid Loss: 0.4867 / \n",
      "Epoch 49 Avg Train Loss: 0.3417 Avg Valid Loss: 0.4549 / \n",
      "Epoch 50 Avg Train Loss: 0.3410 Avg Valid Loss: 0.4440 / \n",
      "========== fold: 0 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.4322\n",
      "========== stage: 2 fold: 0 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.3918 Avg Valid Loss: 0.4221 / \n",
      "Epoch 1 Save Best Valid Loss: 0.4221\n",
      "Epoch 2 Avg Train Loss: 0.3634 Avg Valid Loss: 0.4168 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4168\n",
      "Epoch 3 Avg Train Loss: 0.3590 Avg Valid Loss: 0.4208 / \n",
      "Epoch 4 Avg Train Loss: 0.3547 Avg Valid Loss: 0.4493 / \n",
      "Epoch 5 Avg Train Loss: 0.3495 Avg Valid Loss: 0.4268 / \n",
      "Epoch 6 Avg Train Loss: 0.3407 Avg Valid Loss: 0.4153 / \n",
      "Epoch 6 Save Best Valid Loss: 0.4153\n",
      "Epoch 7 Avg Train Loss: 0.3407 Avg Valid Loss: 0.4052 / \n",
      "Epoch 7 Save Best Valid Loss: 0.4052\n",
      "Epoch 8 Avg Train Loss: 0.3378 Avg Valid Loss: 0.4010 / \n",
      "Epoch 8 Save Best Valid Loss: 0.4010\n",
      "Epoch 9 Avg Train Loss: 0.3351 Avg Valid Loss: 0.3853 / \n",
      "Epoch 9 Save Best Valid Loss: 0.3853\n",
      "Epoch 10 Avg Train Loss: 0.3281 Avg Valid Loss: 0.4141 / \n",
      "Epoch 11 Avg Train Loss: 0.3250 Avg Valid Loss: 0.4621 / \n",
      "Epoch 12 Avg Train Loss: 0.3289 Avg Valid Loss: 0.4118 / \n",
      "Epoch 13 Avg Train Loss: 0.3303 Avg Valid Loss: 0.4124 / \n",
      "Epoch 14 Avg Train Loss: 0.3320 Avg Valid Loss: 0.4031 / \n",
      "Epoch 15 Avg Train Loss: 0.3276 Avg Valid Loss: 0.3906 / \n",
      "Epoch 16 Avg Train Loss: 0.3230 Avg Valid Loss: 0.3777 / \n",
      "Epoch 16 Save Best Valid Loss: 0.3777\n",
      "Epoch 17 Avg Train Loss: 0.3241 Avg Valid Loss: 0.3750 / \n",
      "Epoch 17 Save Best Valid Loss: 0.3750\n",
      "Epoch 18 Avg Train Loss: 0.3254 Avg Valid Loss: 0.3863 / \n",
      "Epoch 19 Avg Train Loss: 0.3216 Avg Valid Loss: 0.3811 / \n",
      "Epoch 20 Avg Train Loss: 0.3198 Avg Valid Loss: 0.3897 / \n",
      "Epoch 21 Avg Train Loss: 0.3214 Avg Valid Loss: 0.3904 / \n",
      "Epoch 22 Avg Train Loss: 0.3273 Avg Valid Loss: 0.3779 / \n",
      "Epoch 23 Avg Train Loss: 0.3210 Avg Valid Loss: 0.3803 / \n",
      "Epoch 24 Avg Train Loss: 0.3195 Avg Valid Loss: 0.3731 / \n",
      "Epoch 24 Save Best Valid Loss: 0.3731\n",
      "Epoch 25 Avg Train Loss: 0.3167 Avg Valid Loss: 0.3753 / \n",
      "Epoch 26 Avg Train Loss: 0.3156 Avg Valid Loss: 0.3829 / \n",
      "Epoch 27 Avg Train Loss: 0.3156 Avg Valid Loss: 0.4160 / \n",
      "Epoch 28 Avg Train Loss: 0.3103 Avg Valid Loss: 0.4130 / \n",
      "Epoch 29 Avg Train Loss: 0.3123 Avg Valid Loss: 0.3831 / \n",
      "Epoch 30 Avg Train Loss: 0.3215 Avg Valid Loss: 0.4068 / \n",
      "Epoch 31 Avg Train Loss: 0.3166 Avg Valid Loss: 0.3771 / \n",
      "Epoch 32 Avg Train Loss: 0.3130 Avg Valid Loss: 0.3946 / \n",
      "Epoch 33 Avg Train Loss: 0.3088 Avg Valid Loss: 0.4250 / \n",
      "Epoch 34 Avg Train Loss: 0.3116 Avg Valid Loss: 0.3829 / \n",
      "Epoch 35 Avg Train Loss: 0.3135 Avg Valid Loss: 0.3824 / \n",
      "Epoch 36 Avg Train Loss: 0.3107 Avg Valid Loss: 0.3964 / \n",
      "Epoch 37 Avg Train Loss: 0.3089 Avg Valid Loss: 0.4120 / \n",
      "Epoch 38 Avg Train Loss: 0.3094 Avg Valid Loss: 0.3810 / \n",
      "Epoch 39 Avg Train Loss: 0.2992 Avg Valid Loss: 0.3737 / \n",
      "Epoch 40 Avg Train Loss: 0.3052 Avg Valid Loss: 0.3618 / \n",
      "Epoch 40 Save Best Valid Loss: 0.3618\n",
      "Epoch 41 Avg Train Loss: 0.3032 Avg Valid Loss: 0.3728 / \n",
      "Epoch 42 Avg Train Loss: 0.3066 Avg Valid Loss: 0.3832 / \n",
      "Epoch 43 Avg Train Loss: 0.3108 Avg Valid Loss: 0.3583 / \n",
      "Epoch 43 Save Best Valid Loss: 0.3583\n",
      "Epoch 44 Avg Train Loss: 0.3029 Avg Valid Loss: 0.3491 / \n",
      "Epoch 44 Save Best Valid Loss: 0.3491\n",
      "Epoch 45 Avg Train Loss: 0.2963 Avg Valid Loss: 0.3554 / \n",
      "Epoch 46 Avg Train Loss: 0.3018 Avg Valid Loss: 0.4075 / \n",
      "Epoch 47 Avg Train Loss: 0.3037 Avg Valid Loss: 0.3633 / \n",
      "Epoch 48 Avg Train Loss: 0.2998 Avg Valid Loss: 0.3975 / \n",
      "Epoch 49 Avg Train Loss: 0.2994 Avg Valid Loss: 0.3673 / \n",
      "Epoch 50 Avg Train Loss: 0.3004 Avg Valid Loss: 0.3573 / \n",
      "Epoch 51 Avg Train Loss: 0.3006 Avg Valid Loss: 0.3621 / \n",
      "Epoch 52 Avg Train Loss: 0.2991 Avg Valid Loss: 0.3683 / \n",
      "Epoch 53 Avg Train Loss: 0.3030 Avg Valid Loss: 0.3833 / \n",
      "Epoch 54 Avg Train Loss: 0.3006 Avg Valid Loss: 0.3610 / \n",
      "Epoch 55 Avg Train Loss: 0.3011 Avg Valid Loss: 0.4155 / \n",
      "Epoch 56 Avg Train Loss: 0.2969 Avg Valid Loss: 0.3586 / \n",
      "Epoch 57 Avg Train Loss: 0.3018 Avg Valid Loss: 0.3744 / \n",
      "Epoch 58 Avg Train Loss: 0.3023 Avg Valid Loss: 0.3720 / \n",
      "Epoch 59 Avg Train Loss: 0.2984 Avg Valid Loss: 0.3673 / \n",
      "Epoch 60 Avg Train Loss: 0.2926 Avg Valid Loss: 0.3949 / \n",
      "Epoch 61 Avg Train Loss: 0.2993 Avg Valid Loss: 0.3743 / \n",
      "Epoch 62 Avg Train Loss: 0.2988 Avg Valid Loss: 0.3586 / \n",
      "Epoch 63 Avg Train Loss: 0.2958 Avg Valid Loss: 0.3352 / \n",
      "Epoch 63 Save Best Valid Loss: 0.3352\n",
      "Epoch 64 Avg Train Loss: 0.3025 Avg Valid Loss: 0.3558 / \n",
      "Epoch 65 Avg Train Loss: 0.3001 Avg Valid Loss: 0.3790 / \n",
      "Epoch 66 Avg Train Loss: 0.2989 Avg Valid Loss: 0.3636 / \n",
      "Epoch 67 Avg Train Loss: 0.2917 Avg Valid Loss: 0.3741 / \n",
      "Epoch 68 Avg Train Loss: 0.2918 Avg Valid Loss: 0.4488 / \n",
      "Epoch 69 Avg Train Loss: 0.2955 Avg Valid Loss: 0.3787 / \n",
      "Epoch 70 Avg Train Loss: 0.2976 Avg Valid Loss: 0.4300 / \n",
      "Epoch 71 Avg Train Loss: 0.2922 Avg Valid Loss: 0.3730 / \n",
      "Epoch 72 Avg Train Loss: 0.2991 Avg Valid Loss: 0.3725 / \n",
      "Epoch 73 Avg Train Loss: 0.2928 Avg Valid Loss: 0.3749 / \n",
      "Epoch 74 Avg Train Loss: 0.2925 Avg Valid Loss: 0.3803 / \n",
      "Epoch 75 Avg Train Loss: 0.2971 Avg Valid Loss: 0.3429 / \n",
      "Epoch 76 Avg Train Loss: 0.2997 Avg Valid Loss: 0.3650 / \n",
      "Epoch 77 Avg Train Loss: 0.2860 Avg Valid Loss: 0.3742 / \n",
      "Epoch 78 Avg Train Loss: 0.2865 Avg Valid Loss: 0.3766 / \n",
      "Epoch 79 Avg Train Loss: 0.2893 Avg Valid Loss: 0.3790 / \n",
      "Epoch 80 Avg Train Loss: 0.2931 Avg Valid Loss: 0.3249 / \n",
      "Epoch 80 Save Best Valid Loss: 0.3249\n",
      "Epoch 81 Avg Train Loss: 0.2931 Avg Valid Loss: 0.3375 / \n",
      "Epoch 82 Avg Train Loss: 0.2881 Avg Valid Loss: 0.3474 / \n",
      "Epoch 83 Avg Train Loss: 0.2898 Avg Valid Loss: 0.3422 / \n",
      "Epoch 84 Avg Train Loss: 0.2870 Avg Valid Loss: 0.3480 / \n",
      "Epoch 85 Avg Train Loss: 0.2880 Avg Valid Loss: 0.3430 / \n",
      "Epoch 86 Avg Train Loss: 0.2874 Avg Valid Loss: 0.3402 / \n",
      "Epoch 87 Avg Train Loss: 0.2823 Avg Valid Loss: 0.3564 / \n",
      "Epoch 88 Avg Train Loss: 0.2877 Avg Valid Loss: 0.3443 / \n",
      "Epoch 89 Avg Train Loss: 0.2831 Avg Valid Loss: 0.3387 / \n",
      "Epoch 90 Avg Train Loss: 0.2902 Avg Valid Loss: 0.3518 / \n",
      "Epoch 91 Avg Train Loss: 0.2827 Avg Valid Loss: 0.3474 / \n",
      "Epoch 92 Avg Train Loss: 0.2917 Avg Valid Loss: 0.3374 / \n",
      "Epoch 93 Avg Train Loss: 0.2842 Avg Valid Loss: 0.3509 / \n",
      "Epoch 94 Avg Train Loss: 0.2870 Avg Valid Loss: 0.3431 / \n",
      "Epoch 95 Avg Train Loss: 0.2901 Avg Valid Loss: 0.3355 / \n",
      "Epoch 96 Avg Train Loss: 0.2874 Avg Valid Loss: 0.3469 / \n",
      "Epoch 97 Avg Train Loss: 0.2913 Avg Valid Loss: 0.3356 / \n",
      "Epoch 98 Avg Train Loss: 0.2862 Avg Valid Loss: 0.3447 / \n",
      "Epoch 99 Avg Train Loss: 0.2875 Avg Valid Loss: 0.3375 / \n",
      "Epoch 100 Avg Train Loss: 0.2869 Avg Valid Loss: 0.3379 / \n",
      "========== fold: 0 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3249\n",
      "========== stage: 1 fold: 1 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5643 Avg Valid Loss: 0.5148 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5148\n",
      "Epoch 2 Avg Train Loss: 0.4792 Avg Valid Loss: 0.4386 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4386\n",
      "Epoch 3 Avg Train Loss: 0.4523 Avg Valid Loss: 0.4565 / \n",
      "Epoch 4 Avg Train Loss: 0.4388 Avg Valid Loss: 0.4394 / \n",
      "Epoch 5 Avg Train Loss: 0.4265 Avg Valid Loss: 0.4514 / \n",
      "Epoch 6 Avg Train Loss: 0.4215 Avg Valid Loss: 0.4245 / \n",
      "Epoch 6 Save Best Valid Loss: 0.4245\n",
      "Epoch 7 Avg Train Loss: 0.4171 Avg Valid Loss: 0.4268 / \n",
      "Epoch 8 Avg Train Loss: 0.4075 Avg Valid Loss: 0.4362 / \n",
      "Epoch 9 Avg Train Loss: 0.4059 Avg Valid Loss: 0.4187 / \n",
      "Epoch 9 Save Best Valid Loss: 0.4187\n",
      "Epoch 10 Avg Train Loss: 0.4032 Avg Valid Loss: 0.4299 / \n",
      "Epoch 11 Avg Train Loss: 0.4021 Avg Valid Loss: 0.4094 / \n",
      "Epoch 11 Save Best Valid Loss: 0.4094\n",
      "Epoch 12 Avg Train Loss: 0.3944 Avg Valid Loss: 0.4070 / \n",
      "Epoch 12 Save Best Valid Loss: 0.4070\n",
      "Epoch 13 Avg Train Loss: 0.3912 Avg Valid Loss: 0.3900 / \n",
      "Epoch 13 Save Best Valid Loss: 0.3900\n",
      "Epoch 14 Avg Train Loss: 0.3865 Avg Valid Loss: 0.4139 / \n",
      "Epoch 15 Avg Train Loss: 0.3838 Avg Valid Loss: 0.3983 / \n",
      "Epoch 16 Avg Train Loss: 0.3856 Avg Valid Loss: 0.4397 / \n",
      "Epoch 17 Avg Train Loss: 0.3810 Avg Valid Loss: 0.4094 / \n",
      "Epoch 18 Avg Train Loss: 0.3791 Avg Valid Loss: 0.3853 / \n",
      "Epoch 18 Save Best Valid Loss: 0.3853\n",
      "Epoch 19 Avg Train Loss: 0.3743 Avg Valid Loss: 0.3889 / \n",
      "Epoch 20 Avg Train Loss: 0.3714 Avg Valid Loss: 0.3869 / \n",
      "Epoch 21 Avg Train Loss: 0.3719 Avg Valid Loss: 0.3989 / \n",
      "Epoch 22 Avg Train Loss: 0.3696 Avg Valid Loss: 0.3749 / \n",
      "Epoch 22 Save Best Valid Loss: 0.3749\n",
      "Epoch 23 Avg Train Loss: 0.3647 Avg Valid Loss: 0.3791 / \n",
      "Epoch 24 Avg Train Loss: 0.3644 Avg Valid Loss: 0.4012 / \n",
      "Epoch 25 Avg Train Loss: 0.3645 Avg Valid Loss: 0.3948 / \n",
      "Epoch 26 Avg Train Loss: 0.3626 Avg Valid Loss: 0.4041 / \n",
      "Epoch 27 Avg Train Loss: 0.3612 Avg Valid Loss: 0.3851 / \n",
      "Epoch 28 Avg Train Loss: 0.3602 Avg Valid Loss: 0.3911 / \n",
      "Epoch 29 Avg Train Loss: 0.3546 Avg Valid Loss: 0.3901 / \n",
      "Epoch 30 Avg Train Loss: 0.3592 Avg Valid Loss: 0.3951 / \n",
      "Epoch 31 Avg Train Loss: 0.3520 Avg Valid Loss: 0.3968 / \n",
      "Epoch 32 Avg Train Loss: 0.3547 Avg Valid Loss: 0.3767 / \n",
      "Epoch 33 Avg Train Loss: 0.3561 Avg Valid Loss: 0.3820 / \n",
      "Epoch 34 Avg Train Loss: 0.3555 Avg Valid Loss: 0.3924 / \n",
      "Epoch 35 Avg Train Loss: 0.3509 Avg Valid Loss: 0.3749 / \n",
      "Epoch 36 Avg Train Loss: 0.3542 Avg Valid Loss: 0.3696 / \n",
      "Epoch 36 Save Best Valid Loss: 0.3696\n",
      "Epoch 37 Avg Train Loss: 0.3468 Avg Valid Loss: 0.3826 / \n",
      "Epoch 38 Avg Train Loss: 0.3521 Avg Valid Loss: 0.3706 / \n",
      "Epoch 39 Avg Train Loss: 0.3461 Avg Valid Loss: 0.3902 / \n",
      "Epoch 40 Avg Train Loss: 0.3487 Avg Valid Loss: 0.3991 / \n",
      "Epoch 41 Avg Train Loss: 0.3509 Avg Valid Loss: 0.3927 / \n",
      "Epoch 42 Avg Train Loss: 0.3516 Avg Valid Loss: 0.3674 / \n",
      "Epoch 42 Save Best Valid Loss: 0.3674\n",
      "Epoch 43 Avg Train Loss: 0.3414 Avg Valid Loss: 0.3935 / \n",
      "Epoch 44 Avg Train Loss: 0.3459 Avg Valid Loss: 0.3746 / \n",
      "Epoch 45 Avg Train Loss: 0.3468 Avg Valid Loss: 0.3618 / \n",
      "Epoch 45 Save Best Valid Loss: 0.3618\n",
      "Epoch 46 Avg Train Loss: 0.3430 Avg Valid Loss: 0.3942 / \n",
      "Epoch 47 Avg Train Loss: 0.3461 Avg Valid Loss: 0.3623 / \n",
      "Epoch 48 Avg Train Loss: 0.3402 Avg Valid Loss: 0.3747 / \n",
      "Epoch 49 Avg Train Loss: 0.3366 Avg Valid Loss: 0.3530 / \n",
      "Epoch 49 Save Best Valid Loss: 0.3530\n",
      "Epoch 50 Avg Train Loss: 0.3384 Avg Valid Loss: 0.3816 / \n",
      "========== fold: 1 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.3530\n",
      "========== stage: 2 fold: 1 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.3886 Avg Valid Loss: 0.3042 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3042\n",
      "Epoch 2 Avg Train Loss: 0.3585 Avg Valid Loss: 0.2931 / \n",
      "Epoch 2 Save Best Valid Loss: 0.2931\n",
      "Epoch 3 Avg Train Loss: 0.3444 Avg Valid Loss: 0.3054 / \n",
      "Epoch 4 Avg Train Loss: 0.3449 Avg Valid Loss: 0.3018 / \n",
      "Epoch 5 Avg Train Loss: 0.3351 Avg Valid Loss: 0.3152 / \n",
      "Epoch 6 Avg Train Loss: 0.3294 Avg Valid Loss: 0.2992 / \n",
      "Epoch 7 Avg Train Loss: 0.3298 Avg Valid Loss: 0.3084 / \n",
      "Epoch 8 Avg Train Loss: 0.3269 Avg Valid Loss: 0.3276 / \n",
      "Epoch 9 Avg Train Loss: 0.3279 Avg Valid Loss: 0.3037 / \n",
      "Epoch 10 Avg Train Loss: 0.3240 Avg Valid Loss: 0.3060 / \n",
      "Epoch 11 Avg Train Loss: 0.3174 Avg Valid Loss: 0.3216 / \n",
      "Epoch 12 Avg Train Loss: 0.3267 Avg Valid Loss: 0.3122 / \n",
      "Epoch 13 Avg Train Loss: 0.3149 Avg Valid Loss: 0.3002 / \n",
      "Epoch 14 Avg Train Loss: 0.3191 Avg Valid Loss: 0.3096 / \n",
      "Epoch 15 Avg Train Loss: 0.3144 Avg Valid Loss: 0.2992 / \n",
      "Epoch 16 Avg Train Loss: 0.3169 Avg Valid Loss: 0.3239 / \n",
      "Epoch 17 Avg Train Loss: 0.3147 Avg Valid Loss: 0.3014 / \n",
      "Epoch 18 Avg Train Loss: 0.3046 Avg Valid Loss: 0.2913 / \n",
      "Epoch 18 Save Best Valid Loss: 0.2913\n",
      "Epoch 19 Avg Train Loss: 0.3046 Avg Valid Loss: 0.3046 / \n",
      "Epoch 20 Avg Train Loss: 0.3099 Avg Valid Loss: 0.3171 / \n",
      "Epoch 21 Avg Train Loss: 0.3063 Avg Valid Loss: 0.2936 / \n",
      "Epoch 22 Avg Train Loss: 0.3094 Avg Valid Loss: 0.2887 / \n",
      "Epoch 22 Save Best Valid Loss: 0.2887\n",
      "Epoch 23 Avg Train Loss: 0.3065 Avg Valid Loss: 0.2996 / \n",
      "Epoch 24 Avg Train Loss: 0.3055 Avg Valid Loss: 0.2917 / \n",
      "Epoch 25 Avg Train Loss: 0.3027 Avg Valid Loss: 0.2930 / \n",
      "Epoch 26 Avg Train Loss: 0.3031 Avg Valid Loss: 0.3032 / \n",
      "Epoch 27 Avg Train Loss: 0.3053 Avg Valid Loss: 0.3089 / \n",
      "Epoch 28 Avg Train Loss: 0.2997 Avg Valid Loss: 0.3294 / \n",
      "Epoch 29 Avg Train Loss: 0.3078 Avg Valid Loss: 0.3171 / \n",
      "Epoch 30 Avg Train Loss: 0.3001 Avg Valid Loss: 0.3061 / \n",
      "Epoch 31 Avg Train Loss: 0.3001 Avg Valid Loss: 0.3218 / \n",
      "Epoch 32 Avg Train Loss: 0.3010 Avg Valid Loss: 0.3194 / \n",
      "Epoch 33 Avg Train Loss: 0.2918 Avg Valid Loss: 0.3036 / \n",
      "Epoch 34 Avg Train Loss: 0.3010 Avg Valid Loss: 0.3101 / \n",
      "Epoch 35 Avg Train Loss: 0.3055 Avg Valid Loss: 0.3141 / \n",
      "Epoch 36 Avg Train Loss: 0.2996 Avg Valid Loss: 0.3095 / \n",
      "Epoch 37 Avg Train Loss: 0.3023 Avg Valid Loss: 0.3078 / \n",
      "Epoch 38 Avg Train Loss: 0.2989 Avg Valid Loss: 0.2970 / \n",
      "Epoch 39 Avg Train Loss: 0.2994 Avg Valid Loss: 0.2934 / \n",
      "Epoch 40 Avg Train Loss: 0.3015 Avg Valid Loss: 0.3004 / \n",
      "Epoch 41 Avg Train Loss: 0.3028 Avg Valid Loss: 0.2931 / \n",
      "Epoch 42 Avg Train Loss: 0.2996 Avg Valid Loss: 0.3049 / \n",
      "Epoch 43 Avg Train Loss: 0.2948 Avg Valid Loss: 0.2999 / \n",
      "Epoch 44 Avg Train Loss: 0.2964 Avg Valid Loss: 0.3153 / \n",
      "Epoch 45 Avg Train Loss: 0.2986 Avg Valid Loss: 0.2934 / \n",
      "Epoch 46 Avg Train Loss: 0.2958 Avg Valid Loss: 0.3209 / \n",
      "Epoch 47 Avg Train Loss: 0.3023 Avg Valid Loss: 0.3081 / \n",
      "Epoch 48 Avg Train Loss: 0.2933 Avg Valid Loss: 0.3031 / \n",
      "Epoch 49 Avg Train Loss: 0.2928 Avg Valid Loss: 0.2928 / \n",
      "Epoch 50 Avg Train Loss: 0.2933 Avg Valid Loss: 0.3065 / \n",
      "Epoch 51 Avg Train Loss: 0.2924 Avg Valid Loss: 0.3299 / \n",
      "Epoch 52 Avg Train Loss: 0.2960 Avg Valid Loss: 0.3210 / \n",
      "Epoch 53 Avg Train Loss: 0.2953 Avg Valid Loss: 0.3217 / \n",
      "Epoch 54 Avg Train Loss: 0.2861 Avg Valid Loss: 0.2934 / \n",
      "Epoch 55 Avg Train Loss: 0.2923 Avg Valid Loss: 0.3026 / \n",
      "Epoch 56 Avg Train Loss: 0.2915 Avg Valid Loss: 0.3017 / \n",
      "Epoch 57 Avg Train Loss: 0.2928 Avg Valid Loss: 0.3003 / \n",
      "Epoch 58 Avg Train Loss: 0.2923 Avg Valid Loss: 0.2970 / \n",
      "Epoch 59 Avg Train Loss: 0.2919 Avg Valid Loss: 0.3027 / \n",
      "Epoch 60 Avg Train Loss: 0.2867 Avg Valid Loss: 0.2948 / \n",
      "Epoch 61 Avg Train Loss: 0.2948 Avg Valid Loss: 0.2964 / \n",
      "Epoch 62 Avg Train Loss: 0.2944 Avg Valid Loss: 0.2974 / \n",
      "Epoch 63 Avg Train Loss: 0.2926 Avg Valid Loss: 0.3115 / \n",
      "Epoch 64 Avg Train Loss: 0.2892 Avg Valid Loss: 0.3124 / \n",
      "Epoch 65 Avg Train Loss: 0.2832 Avg Valid Loss: 0.2996 / \n",
      "Epoch 66 Avg Train Loss: 0.2863 Avg Valid Loss: 0.2988 / \n",
      "Epoch 67 Avg Train Loss: 0.2873 Avg Valid Loss: 0.3165 / \n",
      "Epoch 68 Avg Train Loss: 0.2892 Avg Valid Loss: 0.3429 / \n",
      "Epoch 69 Avg Train Loss: 0.2942 Avg Valid Loss: 0.3145 / \n",
      "Epoch 70 Avg Train Loss: 0.2973 Avg Valid Loss: 0.3113 / \n",
      "Epoch 71 Avg Train Loss: 0.2886 Avg Valid Loss: 0.3074 / \n",
      "Epoch 72 Avg Train Loss: 0.2869 Avg Valid Loss: 0.3094 / \n",
      "Epoch 73 Avg Train Loss: 0.2891 Avg Valid Loss: 0.3058 / \n",
      "Epoch 74 Avg Train Loss: 0.2939 Avg Valid Loss: 0.3079 / \n",
      "Epoch 75 Avg Train Loss: 0.2955 Avg Valid Loss: 0.3064 / \n",
      "Epoch 76 Avg Train Loss: 0.2923 Avg Valid Loss: 0.3120 / \n",
      "Epoch 77 Avg Train Loss: 0.2923 Avg Valid Loss: 0.3125 / \n",
      "Epoch 78 Avg Train Loss: 0.2863 Avg Valid Loss: 0.3114 / \n",
      "Epoch 79 Avg Train Loss: 0.2852 Avg Valid Loss: 0.3082 / \n",
      "Epoch 80 Avg Train Loss: 0.2916 Avg Valid Loss: 0.3078 / \n",
      "Epoch 81 Avg Train Loss: 0.2929 Avg Valid Loss: 0.3091 / \n",
      "Epoch 82 Avg Train Loss: 0.2884 Avg Valid Loss: 0.3078 / \n",
      "Epoch 83 Avg Train Loss: 0.2920 Avg Valid Loss: 0.3090 / \n",
      "Epoch 84 Avg Train Loss: 0.2906 Avg Valid Loss: 0.3073 / \n",
      "Epoch 85 Avg Train Loss: 0.2913 Avg Valid Loss: 0.3128 / \n",
      "Epoch 86 Avg Train Loss: 0.2882 Avg Valid Loss: 0.3065 / \n",
      "Epoch 87 Avg Train Loss: 0.2902 Avg Valid Loss: 0.3146 / \n",
      "Epoch 88 Avg Train Loss: 0.2920 Avg Valid Loss: 0.3048 / \n",
      "Epoch 89 Avg Train Loss: 0.2894 Avg Valid Loss: 0.3087 / \n",
      "Epoch 90 Avg Train Loss: 0.2880 Avg Valid Loss: 0.3066 / \n",
      "Epoch 91 Avg Train Loss: 0.2933 Avg Valid Loss: 0.3068 / \n",
      "Epoch 92 Avg Train Loss: 0.2925 Avg Valid Loss: 0.3089 / \n",
      "Epoch 93 Avg Train Loss: 0.2946 Avg Valid Loss: 0.3114 / \n",
      "Epoch 94 Avg Train Loss: 0.2846 Avg Valid Loss: 0.3130 / \n",
      "Epoch 95 Avg Train Loss: 0.2845 Avg Valid Loss: 0.3072 / \n",
      "Epoch 96 Avg Train Loss: 0.2960 Avg Valid Loss: 0.3144 / \n",
      "Epoch 97 Avg Train Loss: 0.2903 Avg Valid Loss: 0.3087 / \n",
      "Epoch 98 Avg Train Loss: 0.2916 Avg Valid Loss: 0.3120 / \n",
      "Epoch 99 Avg Train Loss: 0.2905 Avg Valid Loss: 0.3131 / \n",
      "Epoch 100 Avg Train Loss: 0.2903 Avg Valid Loss: 0.3074 / \n",
      "========== fold: 1 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.2887\n",
      "========== stage: 1 fold: 2 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5481 Avg Valid Loss: 0.4544 / \n",
      "Epoch 1 Save Best Valid Loss: 0.4544\n",
      "Epoch 2 Avg Train Loss: 0.4833 Avg Valid Loss: 0.4294 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4294\n",
      "Epoch 3 Avg Train Loss: 0.4557 Avg Valid Loss: 0.4090 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4090\n",
      "Epoch 4 Avg Train Loss: 0.4473 Avg Valid Loss: 0.3896 / \n",
      "Epoch 4 Save Best Valid Loss: 0.3896\n",
      "Epoch 5 Avg Train Loss: 0.4368 Avg Valid Loss: 0.3899 / \n",
      "Epoch 6 Avg Train Loss: 0.4287 Avg Valid Loss: 0.4044 / \n",
      "Epoch 7 Avg Train Loss: 0.4211 Avg Valid Loss: 0.3835 / \n",
      "Epoch 7 Save Best Valid Loss: 0.3835\n",
      "Epoch 8 Avg Train Loss: 0.4188 Avg Valid Loss: 0.3735 / \n",
      "Epoch 8 Save Best Valid Loss: 0.3735\n",
      "Epoch 9 Avg Train Loss: 0.4138 Avg Valid Loss: 0.3730 / \n",
      "Epoch 9 Save Best Valid Loss: 0.3730\n",
      "Epoch 10 Avg Train Loss: 0.4059 Avg Valid Loss: 0.3815 / \n",
      "Epoch 11 Avg Train Loss: 0.4056 Avg Valid Loss: 0.3803 / \n",
      "Epoch 12 Avg Train Loss: 0.4034 Avg Valid Loss: 0.3970 / \n",
      "Epoch 13 Avg Train Loss: 0.3994 Avg Valid Loss: 0.3819 / \n",
      "Epoch 14 Avg Train Loss: 0.3967 Avg Valid Loss: 0.4156 / \n",
      "Epoch 15 Avg Train Loss: 0.3967 Avg Valid Loss: 0.3654 / \n",
      "Epoch 15 Save Best Valid Loss: 0.3654\n",
      "Epoch 16 Avg Train Loss: 0.3962 Avg Valid Loss: 0.3639 / \n",
      "Epoch 16 Save Best Valid Loss: 0.3639\n",
      "Epoch 17 Avg Train Loss: 0.3884 Avg Valid Loss: 0.3693 / \n",
      "Epoch 18 Avg Train Loss: 0.3916 Avg Valid Loss: 0.3736 / \n",
      "Epoch 19 Avg Train Loss: 0.3893 Avg Valid Loss: 0.3807 / \n",
      "Epoch 20 Avg Train Loss: 0.3834 Avg Valid Loss: 0.3445 / \n",
      "Epoch 20 Save Best Valid Loss: 0.3445\n",
      "Epoch 21 Avg Train Loss: 0.3846 Avg Valid Loss: 0.3864 / \n",
      "Epoch 22 Avg Train Loss: 0.3853 Avg Valid Loss: 0.3670 / \n",
      "Epoch 23 Avg Train Loss: 0.3789 Avg Valid Loss: 0.3702 / \n",
      "Epoch 24 Avg Train Loss: 0.3815 Avg Valid Loss: 0.3739 / \n",
      "Epoch 25 Avg Train Loss: 0.3792 Avg Valid Loss: 0.3676 / \n",
      "Epoch 26 Avg Train Loss: 0.3767 Avg Valid Loss: 0.3527 / \n",
      "Epoch 27 Avg Train Loss: 0.3749 Avg Valid Loss: 0.3455 / \n",
      "Epoch 28 Avg Train Loss: 0.3701 Avg Valid Loss: 0.3493 / \n",
      "Epoch 29 Avg Train Loss: 0.3710 Avg Valid Loss: 0.3511 / \n",
      "Epoch 30 Avg Train Loss: 0.3685 Avg Valid Loss: 0.3653 / \n",
      "Epoch 31 Avg Train Loss: 0.3709 Avg Valid Loss: 0.3419 / \n",
      "Epoch 31 Save Best Valid Loss: 0.3419\n",
      "Epoch 32 Avg Train Loss: 0.3705 Avg Valid Loss: 0.3743 / \n",
      "Epoch 33 Avg Train Loss: 0.3678 Avg Valid Loss: 0.3483 / \n",
      "Epoch 34 Avg Train Loss: 0.3674 Avg Valid Loss: 0.3547 / \n",
      "Epoch 35 Avg Train Loss: 0.3664 Avg Valid Loss: 0.3438 / \n",
      "Epoch 36 Avg Train Loss: 0.3613 Avg Valid Loss: 0.3613 / \n",
      "Epoch 37 Avg Train Loss: 0.3667 Avg Valid Loss: 0.3498 / \n",
      "Epoch 38 Avg Train Loss: 0.3616 Avg Valid Loss: 0.3298 / \n",
      "Epoch 38 Save Best Valid Loss: 0.3298\n",
      "Epoch 39 Avg Train Loss: 0.3599 Avg Valid Loss: 0.3355 / \n",
      "Epoch 40 Avg Train Loss: 0.3627 Avg Valid Loss: 0.3427 / \n",
      "Epoch 41 Avg Train Loss: 0.3610 Avg Valid Loss: 0.3706 / \n",
      "Epoch 42 Avg Train Loss: 0.3553 Avg Valid Loss: 0.3428 / \n",
      "Epoch 43 Avg Train Loss: 0.3587 Avg Valid Loss: 0.3617 / \n",
      "Epoch 44 Avg Train Loss: 0.3592 Avg Valid Loss: 0.3443 / \n",
      "Epoch 45 Avg Train Loss: 0.3539 Avg Valid Loss: 0.3372 / \n",
      "Epoch 46 Avg Train Loss: 0.3589 Avg Valid Loss: 0.3450 / \n",
      "Epoch 47 Avg Train Loss: 0.3555 Avg Valid Loss: 0.3333 / \n",
      "Epoch 48 Avg Train Loss: 0.3565 Avg Valid Loss: 0.3455 / \n",
      "Epoch 49 Avg Train Loss: 0.3515 Avg Valid Loss: 0.3334 / \n",
      "Epoch 50 Avg Train Loss: 0.3557 Avg Valid Loss: 0.3516 / \n",
      "========== fold: 2 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.3298\n",
      "========== stage: 2 fold: 2 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.4053 Avg Valid Loss: 0.3326 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3326\n",
      "Epoch 2 Avg Train Loss: 0.3684 Avg Valid Loss: 0.3308 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3308\n",
      "Epoch 3 Avg Train Loss: 0.3502 Avg Valid Loss: 0.3477 / \n",
      "Epoch 4 Avg Train Loss: 0.3529 Avg Valid Loss: 0.3415 / \n",
      "Epoch 5 Avg Train Loss: 0.3485 Avg Valid Loss: 0.3464 / \n",
      "Epoch 6 Avg Train Loss: 0.3432 Avg Valid Loss: 0.3304 / \n",
      "Epoch 6 Save Best Valid Loss: 0.3304\n",
      "Epoch 7 Avg Train Loss: 0.3428 Avg Valid Loss: 0.3380 / \n",
      "Epoch 8 Avg Train Loss: 0.3439 Avg Valid Loss: 0.3246 / \n",
      "Epoch 8 Save Best Valid Loss: 0.3246\n",
      "Epoch 9 Avg Train Loss: 0.3355 Avg Valid Loss: 0.3335 / \n",
      "Epoch 10 Avg Train Loss: 0.3337 Avg Valid Loss: 0.3321 / \n",
      "Epoch 11 Avg Train Loss: 0.3378 Avg Valid Loss: 0.3366 / \n",
      "Epoch 12 Avg Train Loss: 0.3362 Avg Valid Loss: 0.3356 / \n",
      "Epoch 13 Avg Train Loss: 0.3268 Avg Valid Loss: 0.3255 / \n",
      "Epoch 14 Avg Train Loss: 0.3308 Avg Valid Loss: 0.3213 / \n",
      "Epoch 14 Save Best Valid Loss: 0.3213\n",
      "Epoch 15 Avg Train Loss: 0.3192 Avg Valid Loss: 0.3249 / \n",
      "Epoch 16 Avg Train Loss: 0.3324 Avg Valid Loss: 0.3111 / \n",
      "Epoch 16 Save Best Valid Loss: 0.3111\n",
      "Epoch 17 Avg Train Loss: 0.3278 Avg Valid Loss: 0.3118 / \n",
      "Epoch 18 Avg Train Loss: 0.3238 Avg Valid Loss: 0.3176 / \n",
      "Epoch 19 Avg Train Loss: 0.3275 Avg Valid Loss: 0.3127 / \n",
      "Epoch 20 Avg Train Loss: 0.3250 Avg Valid Loss: 0.3360 / \n",
      "Epoch 21 Avg Train Loss: 0.3230 Avg Valid Loss: 0.3222 / \n",
      "Epoch 22 Avg Train Loss: 0.3169 Avg Valid Loss: 0.3242 / \n",
      "Epoch 23 Avg Train Loss: 0.3125 Avg Valid Loss: 0.3120 / \n",
      "Epoch 24 Avg Train Loss: 0.3141 Avg Valid Loss: 0.3250 / \n",
      "Epoch 25 Avg Train Loss: 0.3213 Avg Valid Loss: 0.3370 / \n",
      "Epoch 26 Avg Train Loss: 0.3139 Avg Valid Loss: 0.3389 / \n",
      "Epoch 27 Avg Train Loss: 0.3166 Avg Valid Loss: 0.3309 / \n",
      "Epoch 28 Avg Train Loss: 0.3149 Avg Valid Loss: 0.3043 / \n",
      "Epoch 28 Save Best Valid Loss: 0.3043\n",
      "Epoch 29 Avg Train Loss: 0.3205 Avg Valid Loss: 0.3264 / \n",
      "Epoch 30 Avg Train Loss: 0.3154 Avg Valid Loss: 0.3292 / \n",
      "Epoch 31 Avg Train Loss: 0.3110 Avg Valid Loss: 0.3087 / \n",
      "Epoch 32 Avg Train Loss: 0.3121 Avg Valid Loss: 0.3246 / \n",
      "Epoch 33 Avg Train Loss: 0.3095 Avg Valid Loss: 0.3162 / \n",
      "Epoch 34 Avg Train Loss: 0.3134 Avg Valid Loss: 0.3221 / \n",
      "Epoch 35 Avg Train Loss: 0.3057 Avg Valid Loss: 0.3319 / \n",
      "Epoch 36 Avg Train Loss: 0.3065 Avg Valid Loss: 0.3174 / \n",
      "Epoch 37 Avg Train Loss: 0.3055 Avg Valid Loss: 0.3144 / \n",
      "Epoch 38 Avg Train Loss: 0.3073 Avg Valid Loss: 0.3171 / \n",
      "Epoch 39 Avg Train Loss: 0.2995 Avg Valid Loss: 0.3065 / \n",
      "Epoch 40 Avg Train Loss: 0.3091 Avg Valid Loss: 0.2973 / \n",
      "Epoch 40 Save Best Valid Loss: 0.2973\n",
      "Epoch 41 Avg Train Loss: 0.3066 Avg Valid Loss: 0.3074 / \n",
      "Epoch 42 Avg Train Loss: 0.3033 Avg Valid Loss: 0.2980 / \n",
      "Epoch 43 Avg Train Loss: 0.3032 Avg Valid Loss: 0.3086 / \n",
      "Epoch 44 Avg Train Loss: 0.3043 Avg Valid Loss: 0.3053 / \n",
      "Epoch 45 Avg Train Loss: 0.3013 Avg Valid Loss: 0.3147 / \n",
      "Epoch 46 Avg Train Loss: 0.2931 Avg Valid Loss: 0.3273 / \n",
      "Epoch 47 Avg Train Loss: 0.2975 Avg Valid Loss: 0.3276 / \n",
      "Epoch 48 Avg Train Loss: 0.3022 Avg Valid Loss: 0.3226 / \n",
      "Epoch 49 Avg Train Loss: 0.3008 Avg Valid Loss: 0.3244 / \n",
      "Epoch 50 Avg Train Loss: 0.3023 Avg Valid Loss: 0.3152 / \n",
      "Epoch 51 Avg Train Loss: 0.2981 Avg Valid Loss: 0.3059 / \n",
      "Epoch 52 Avg Train Loss: 0.3030 Avg Valid Loss: 0.3197 / \n",
      "Epoch 53 Avg Train Loss: 0.2988 Avg Valid Loss: 0.3283 / \n",
      "Epoch 54 Avg Train Loss: 0.3020 Avg Valid Loss: 0.3148 / \n",
      "Epoch 55 Avg Train Loss: 0.3015 Avg Valid Loss: 0.3167 / \n",
      "Epoch 56 Avg Train Loss: 0.2956 Avg Valid Loss: 0.3400 / \n",
      "Epoch 57 Avg Train Loss: 0.2923 Avg Valid Loss: 0.3178 / \n",
      "Epoch 58 Avg Train Loss: 0.3088 Avg Valid Loss: 0.3276 / \n",
      "Epoch 59 Avg Train Loss: 0.2980 Avg Valid Loss: 0.3267 / \n",
      "Epoch 60 Avg Train Loss: 0.2999 Avg Valid Loss: 0.3172 / \n",
      "Epoch 61 Avg Train Loss: 0.2939 Avg Valid Loss: 0.3218 / \n",
      "Epoch 62 Avg Train Loss: 0.2988 Avg Valid Loss: 0.3231 / \n",
      "Epoch 63 Avg Train Loss: 0.3003 Avg Valid Loss: 0.3148 / \n",
      "Epoch 64 Avg Train Loss: 0.2947 Avg Valid Loss: 0.3136 / \n",
      "Epoch 65 Avg Train Loss: 0.2979 Avg Valid Loss: 0.3165 / \n",
      "Epoch 66 Avg Train Loss: 0.2947 Avg Valid Loss: 0.3308 / \n",
      "Epoch 67 Avg Train Loss: 0.2946 Avg Valid Loss: 0.3230 / \n",
      "Epoch 68 Avg Train Loss: 0.2913 Avg Valid Loss: 0.3255 / \n",
      "Epoch 69 Avg Train Loss: 0.2859 Avg Valid Loss: 0.3189 / \n",
      "Epoch 70 Avg Train Loss: 0.2957 Avg Valid Loss: 0.3459 / \n",
      "Epoch 71 Avg Train Loss: 0.2963 Avg Valid Loss: 0.3230 / \n",
      "Epoch 72 Avg Train Loss: 0.2940 Avg Valid Loss: 0.3107 / \n",
      "Epoch 73 Avg Train Loss: 0.2926 Avg Valid Loss: 0.3194 / \n",
      "Epoch 74 Avg Train Loss: 0.2905 Avg Valid Loss: 0.3127 / \n",
      "Epoch 75 Avg Train Loss: 0.2965 Avg Valid Loss: 0.3181 / \n",
      "Epoch 76 Avg Train Loss: 0.2957 Avg Valid Loss: 0.3236 / \n",
      "Epoch 77 Avg Train Loss: 0.2967 Avg Valid Loss: 0.3112 / \n",
      "Epoch 78 Avg Train Loss: 0.2895 Avg Valid Loss: 0.3105 / \n",
      "Epoch 79 Avg Train Loss: 0.2882 Avg Valid Loss: 0.3215 / \n",
      "Epoch 80 Avg Train Loss: 0.2913 Avg Valid Loss: 0.3201 / \n",
      "Epoch 81 Avg Train Loss: 0.2884 Avg Valid Loss: 0.3209 / \n",
      "Epoch 82 Avg Train Loss: 0.2854 Avg Valid Loss: 0.3206 / \n",
      "Epoch 83 Avg Train Loss: 0.2867 Avg Valid Loss: 0.3199 / \n",
      "Epoch 84 Avg Train Loss: 0.2869 Avg Valid Loss: 0.3203 / \n",
      "Epoch 85 Avg Train Loss: 0.2893 Avg Valid Loss: 0.3222 / \n",
      "Epoch 86 Avg Train Loss: 0.2936 Avg Valid Loss: 0.3211 / \n",
      "Epoch 87 Avg Train Loss: 0.2912 Avg Valid Loss: 0.3208 / \n",
      "Epoch 88 Avg Train Loss: 0.2837 Avg Valid Loss: 0.3203 / \n",
      "Epoch 89 Avg Train Loss: 0.2850 Avg Valid Loss: 0.3228 / \n",
      "Epoch 90 Avg Train Loss: 0.2849 Avg Valid Loss: 0.3192 / \n",
      "Epoch 91 Avg Train Loss: 0.2837 Avg Valid Loss: 0.3219 / \n",
      "Epoch 92 Avg Train Loss: 0.2861 Avg Valid Loss: 0.3230 / \n",
      "Epoch 93 Avg Train Loss: 0.2878 Avg Valid Loss: 0.3216 / \n",
      "Epoch 94 Avg Train Loss: 0.2889 Avg Valid Loss: 0.3206 / \n",
      "Epoch 95 Avg Train Loss: 0.2879 Avg Valid Loss: 0.3191 / \n",
      "Epoch 96 Avg Train Loss: 0.2902 Avg Valid Loss: 0.3213 / \n",
      "Epoch 97 Avg Train Loss: 0.2852 Avg Valid Loss: 0.3207 / \n",
      "Epoch 98 Avg Train Loss: 0.2887 Avg Valid Loss: 0.3232 / \n",
      "Epoch 99 Avg Train Loss: 0.2936 Avg Valid Loss: 0.3195 / \n",
      "Epoch 100 Avg Train Loss: 0.2889 Avg Valid Loss: 0.3213 / \n",
      "========== fold: 2 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.2973\n",
      "========== stage: 1 fold: 3 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5542 Avg Valid Loss: 0.5522 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5522\n",
      "Epoch 2 Avg Train Loss: 0.4763 Avg Valid Loss: 0.4581 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4581\n",
      "Epoch 3 Avg Train Loss: 0.4515 Avg Valid Loss: 0.4914 / \n",
      "Epoch 4 Avg Train Loss: 0.4434 Avg Valid Loss: 0.4434 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4434\n",
      "Epoch 5 Avg Train Loss: 0.4259 Avg Valid Loss: 0.4812 / \n",
      "Epoch 6 Avg Train Loss: 0.4198 Avg Valid Loss: 0.4362 / \n",
      "Epoch 6 Save Best Valid Loss: 0.4362\n",
      "Epoch 7 Avg Train Loss: 0.4175 Avg Valid Loss: 0.4184 / \n",
      "Epoch 7 Save Best Valid Loss: 0.4184\n",
      "Epoch 8 Avg Train Loss: 0.4127 Avg Valid Loss: 0.4185 / \n",
      "Epoch 9 Avg Train Loss: 0.4015 Avg Valid Loss: 0.4162 / \n",
      "Epoch 9 Save Best Valid Loss: 0.4162\n",
      "Epoch 10 Avg Train Loss: 0.4032 Avg Valid Loss: 0.4399 / \n",
      "Epoch 11 Avg Train Loss: 0.3971 Avg Valid Loss: 0.4424 / \n",
      "Epoch 12 Avg Train Loss: 0.3929 Avg Valid Loss: 0.4114 / \n",
      "Epoch 12 Save Best Valid Loss: 0.4114\n",
      "Epoch 13 Avg Train Loss: 0.3885 Avg Valid Loss: 0.4061 / \n",
      "Epoch 13 Save Best Valid Loss: 0.4061\n",
      "Epoch 14 Avg Train Loss: 0.3856 Avg Valid Loss: 0.4109 / \n",
      "Epoch 15 Avg Train Loss: 0.3859 Avg Valid Loss: 0.3926 / \n",
      "Epoch 15 Save Best Valid Loss: 0.3926\n",
      "Epoch 16 Avg Train Loss: 0.3835 Avg Valid Loss: 0.4161 / \n",
      "Epoch 17 Avg Train Loss: 0.3838 Avg Valid Loss: 0.3970 / \n",
      "Epoch 18 Avg Train Loss: 0.3806 Avg Valid Loss: 0.4005 / \n",
      "Epoch 19 Avg Train Loss: 0.3783 Avg Valid Loss: 0.3982 / \n",
      "Epoch 20 Avg Train Loss: 0.3735 Avg Valid Loss: 0.3972 / \n",
      "Epoch 21 Avg Train Loss: 0.3773 Avg Valid Loss: 0.4121 / \n",
      "Epoch 22 Avg Train Loss: 0.3735 Avg Valid Loss: 0.3905 / \n",
      "Epoch 22 Save Best Valid Loss: 0.3905\n",
      "Epoch 23 Avg Train Loss: 0.3708 Avg Valid Loss: 0.4186 / \n",
      "Epoch 24 Avg Train Loss: 0.3676 Avg Valid Loss: 0.4100 / \n",
      "Epoch 25 Avg Train Loss: 0.3637 Avg Valid Loss: 0.3847 / \n",
      "Epoch 25 Save Best Valid Loss: 0.3847\n",
      "Epoch 26 Avg Train Loss: 0.3619 Avg Valid Loss: 0.3903 / \n",
      "Epoch 27 Avg Train Loss: 0.3681 Avg Valid Loss: 0.4010 / \n",
      "Epoch 28 Avg Train Loss: 0.3686 Avg Valid Loss: 0.3906 / \n",
      "Epoch 29 Avg Train Loss: 0.3670 Avg Valid Loss: 0.4012 / \n",
      "Epoch 30 Avg Train Loss: 0.3652 Avg Valid Loss: 0.4399 / \n",
      "Epoch 31 Avg Train Loss: 0.3632 Avg Valid Loss: 0.3803 / \n",
      "Epoch 31 Save Best Valid Loss: 0.3803\n",
      "Epoch 32 Avg Train Loss: 0.3627 Avg Valid Loss: 0.4209 / \n",
      "Epoch 33 Avg Train Loss: 0.3550 Avg Valid Loss: 0.3869 / \n",
      "Epoch 34 Avg Train Loss: 0.3636 Avg Valid Loss: 0.4050 / \n",
      "Epoch 35 Avg Train Loss: 0.3567 Avg Valid Loss: 0.3847 / \n",
      "Epoch 36 Avg Train Loss: 0.3541 Avg Valid Loss: 0.3811 / \n",
      "Epoch 37 Avg Train Loss: 0.3527 Avg Valid Loss: 0.3898 / \n",
      "Epoch 38 Avg Train Loss: 0.3528 Avg Valid Loss: 0.3907 / \n",
      "Epoch 39 Avg Train Loss: 0.3534 Avg Valid Loss: 0.4050 / \n",
      "Epoch 40 Avg Train Loss: 0.3551 Avg Valid Loss: 0.3999 / \n",
      "Epoch 41 Avg Train Loss: 0.3488 Avg Valid Loss: 0.3928 / \n",
      "Epoch 42 Avg Train Loss: 0.3493 Avg Valid Loss: 0.3782 / \n",
      "Epoch 42 Save Best Valid Loss: 0.3782\n",
      "Epoch 43 Avg Train Loss: 0.3532 Avg Valid Loss: 0.3994 / \n",
      "Epoch 44 Avg Train Loss: 0.3489 Avg Valid Loss: 0.3948 / \n",
      "Epoch 45 Avg Train Loss: 0.3466 Avg Valid Loss: 0.3900 / \n",
      "Epoch 46 Avg Train Loss: 0.3534 Avg Valid Loss: 0.3903 / \n",
      "Epoch 47 Avg Train Loss: 0.3501 Avg Valid Loss: 0.3831 / \n",
      "Epoch 48 Avg Train Loss: 0.3477 Avg Valid Loss: 0.3827 / \n",
      "Epoch 49 Avg Train Loss: 0.3472 Avg Valid Loss: 0.3767 / \n",
      "Epoch 49 Save Best Valid Loss: 0.3767\n",
      "Epoch 50 Avg Train Loss: 0.3424 Avg Valid Loss: 0.4024 / \n",
      "========== fold: 3 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.3767\n",
      "========== stage: 2 fold: 3 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.3975 Avg Valid Loss: 0.3583 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3583\n",
      "Epoch 2 Avg Train Loss: 0.3603 Avg Valid Loss: 0.3431 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3431\n",
      "Epoch 3 Avg Train Loss: 0.3477 Avg Valid Loss: 0.3582 / \n",
      "Epoch 4 Avg Train Loss: 0.3444 Avg Valid Loss: 0.3450 / \n",
      "Epoch 5 Avg Train Loss: 0.3354 Avg Valid Loss: 0.3467 / \n",
      "Epoch 6 Avg Train Loss: 0.3379 Avg Valid Loss: 0.3437 / \n",
      "Epoch 7 Avg Train Loss: 0.3288 Avg Valid Loss: 0.3462 / \n",
      "Epoch 8 Avg Train Loss: 0.3248 Avg Valid Loss: 0.3364 / \n",
      "Epoch 8 Save Best Valid Loss: 0.3364\n",
      "Epoch 9 Avg Train Loss: 0.3275 Avg Valid Loss: 0.3488 / \n",
      "Epoch 10 Avg Train Loss: 0.3194 Avg Valid Loss: 0.3288 / \n",
      "Epoch 10 Save Best Valid Loss: 0.3288\n",
      "Epoch 11 Avg Train Loss: 0.3168 Avg Valid Loss: 0.3445 / \n",
      "Epoch 12 Avg Train Loss: 0.3166 Avg Valid Loss: 0.3789 / \n",
      "Epoch 13 Avg Train Loss: 0.3161 Avg Valid Loss: 0.3376 / \n",
      "Epoch 14 Avg Train Loss: 0.3163 Avg Valid Loss: 0.3279 / \n",
      "Epoch 14 Save Best Valid Loss: 0.3279\n",
      "Epoch 15 Avg Train Loss: 0.3182 Avg Valid Loss: 0.3591 / \n",
      "Epoch 16 Avg Train Loss: 0.3212 Avg Valid Loss: 0.3715 / \n",
      "Epoch 17 Avg Train Loss: 0.3132 Avg Valid Loss: 0.3267 / \n",
      "Epoch 17 Save Best Valid Loss: 0.3267\n",
      "Epoch 18 Avg Train Loss: 0.3119 Avg Valid Loss: 0.3208 / \n",
      "Epoch 18 Save Best Valid Loss: 0.3208\n",
      "Epoch 19 Avg Train Loss: 0.3116 Avg Valid Loss: 0.3387 / \n",
      "Epoch 20 Avg Train Loss: 0.3100 Avg Valid Loss: 0.3327 / \n",
      "Epoch 21 Avg Train Loss: 0.3133 Avg Valid Loss: 0.3241 / \n",
      "Epoch 22 Avg Train Loss: 0.3096 Avg Valid Loss: 0.3379 / \n",
      "Epoch 23 Avg Train Loss: 0.3049 Avg Valid Loss: 0.3536 / \n",
      "Epoch 24 Avg Train Loss: 0.3009 Avg Valid Loss: 0.3159 / \n",
      "Epoch 24 Save Best Valid Loss: 0.3159\n",
      "Epoch 25 Avg Train Loss: 0.3056 Avg Valid Loss: 0.3701 / \n",
      "Epoch 26 Avg Train Loss: 0.3035 Avg Valid Loss: 0.3272 / \n",
      "Epoch 27 Avg Train Loss: 0.3016 Avg Valid Loss: 0.3407 / \n",
      "Epoch 28 Avg Train Loss: 0.3053 Avg Valid Loss: 0.3233 / \n",
      "Epoch 29 Avg Train Loss: 0.3066 Avg Valid Loss: 0.3484 / \n",
      "Epoch 30 Avg Train Loss: 0.2995 Avg Valid Loss: 0.3830 / \n",
      "Epoch 31 Avg Train Loss: 0.2965 Avg Valid Loss: 0.3449 / \n",
      "Epoch 32 Avg Train Loss: 0.3028 Avg Valid Loss: 0.3491 / \n",
      "Epoch 33 Avg Train Loss: 0.2979 Avg Valid Loss: 0.3505 / \n",
      "Epoch 34 Avg Train Loss: 0.2960 Avg Valid Loss: 0.3438 / \n",
      "Epoch 35 Avg Train Loss: 0.2967 Avg Valid Loss: 0.3280 / \n",
      "Epoch 36 Avg Train Loss: 0.3015 Avg Valid Loss: 0.3148 / \n",
      "Epoch 36 Save Best Valid Loss: 0.3148\n",
      "Epoch 37 Avg Train Loss: 0.2970 Avg Valid Loss: 0.3394 / \n",
      "Epoch 38 Avg Train Loss: 0.3012 Avg Valid Loss: 0.3229 / \n",
      "Epoch 39 Avg Train Loss: 0.2942 Avg Valid Loss: 0.3347 / \n",
      "Epoch 40 Avg Train Loss: 0.2963 Avg Valid Loss: 0.3298 / \n",
      "Epoch 41 Avg Train Loss: 0.2899 Avg Valid Loss: 0.3122 / \n",
      "Epoch 41 Save Best Valid Loss: 0.3122\n",
      "Epoch 42 Avg Train Loss: 0.2935 Avg Valid Loss: 0.3192 / \n",
      "Epoch 43 Avg Train Loss: 0.2966 Avg Valid Loss: 0.3145 / \n",
      "Epoch 44 Avg Train Loss: 0.2928 Avg Valid Loss: 0.3221 / \n",
      "Epoch 45 Avg Train Loss: 0.2970 Avg Valid Loss: 0.3327 / \n",
      "Epoch 46 Avg Train Loss: 0.2888 Avg Valid Loss: 0.3262 / \n",
      "Epoch 47 Avg Train Loss: 0.2878 Avg Valid Loss: 0.3435 / \n",
      "Epoch 48 Avg Train Loss: 0.2924 Avg Valid Loss: 0.3450 / \n",
      "Epoch 49 Avg Train Loss: 0.2886 Avg Valid Loss: 0.3555 / \n",
      "Epoch 50 Avg Train Loss: 0.2874 Avg Valid Loss: 0.3493 / \n",
      "Epoch 51 Avg Train Loss: 0.2876 Avg Valid Loss: 0.3215 / \n",
      "Epoch 52 Avg Train Loss: 0.2933 Avg Valid Loss: 0.3300 / \n",
      "Epoch 53 Avg Train Loss: 0.2878 Avg Valid Loss: 0.3368 / \n",
      "Epoch 54 Avg Train Loss: 0.2871 Avg Valid Loss: 0.3133 / \n",
      "Epoch 55 Avg Train Loss: 0.2867 Avg Valid Loss: 0.3142 / \n",
      "Epoch 56 Avg Train Loss: 0.2867 Avg Valid Loss: 0.3313 / \n",
      "Epoch 57 Avg Train Loss: 0.2879 Avg Valid Loss: 0.3092 / \n",
      "Epoch 57 Save Best Valid Loss: 0.3092\n",
      "Epoch 58 Avg Train Loss: 0.2853 Avg Valid Loss: 0.3361 / \n",
      "Epoch 59 Avg Train Loss: 0.2881 Avg Valid Loss: 0.3199 / \n",
      "Epoch 60 Avg Train Loss: 0.2862 Avg Valid Loss: 0.3314 / \n",
      "Epoch 61 Avg Train Loss: 0.2899 Avg Valid Loss: 0.3112 / \n",
      "Epoch 62 Avg Train Loss: 0.2915 Avg Valid Loss: 0.3225 / \n",
      "Epoch 63 Avg Train Loss: 0.2867 Avg Valid Loss: 0.3425 / \n",
      "Epoch 64 Avg Train Loss: 0.2843 Avg Valid Loss: 0.3204 / \n",
      "Epoch 65 Avg Train Loss: 0.2867 Avg Valid Loss: 0.3197 / \n",
      "Epoch 66 Avg Train Loss: 0.2849 Avg Valid Loss: 0.3073 / \n",
      "Epoch 66 Save Best Valid Loss: 0.3073\n",
      "Epoch 67 Avg Train Loss: 0.2835 Avg Valid Loss: 0.3524 / \n",
      "Epoch 68 Avg Train Loss: 0.2897 Avg Valid Loss: 0.3126 / \n",
      "Epoch 69 Avg Train Loss: 0.2829 Avg Valid Loss: 0.3349 / \n",
      "Epoch 70 Avg Train Loss: 0.2769 Avg Valid Loss: 0.3163 / \n",
      "Epoch 71 Avg Train Loss: 0.2828 Avg Valid Loss: 0.3340 / \n",
      "Epoch 72 Avg Train Loss: 0.2890 Avg Valid Loss: 0.3316 / \n",
      "Epoch 73 Avg Train Loss: 0.2784 Avg Valid Loss: 0.3483 / \n",
      "Epoch 74 Avg Train Loss: 0.2790 Avg Valid Loss: 0.3317 / \n",
      "Epoch 75 Avg Train Loss: 0.2818 Avg Valid Loss: 0.3333 / \n",
      "Epoch 76 Avg Train Loss: 0.2822 Avg Valid Loss: 0.3306 / \n",
      "Epoch 77 Avg Train Loss: 0.2886 Avg Valid Loss: 0.3046 / \n",
      "Epoch 77 Save Best Valid Loss: 0.3046\n",
      "Epoch 78 Avg Train Loss: 0.2794 Avg Valid Loss: 0.3273 / \n",
      "Epoch 79 Avg Train Loss: 0.2785 Avg Valid Loss: 0.3162 / \n",
      "Epoch 80 Avg Train Loss: 0.2733 Avg Valid Loss: 0.3254 / \n",
      "Epoch 81 Avg Train Loss: 0.2803 Avg Valid Loss: 0.3156 / \n",
      "Epoch 82 Avg Train Loss: 0.2791 Avg Valid Loss: 0.3025 / \n",
      "Epoch 82 Save Best Valid Loss: 0.3025\n",
      "Epoch 83 Avg Train Loss: 0.2815 Avg Valid Loss: 0.3257 / \n",
      "Epoch 84 Avg Train Loss: 0.2796 Avg Valid Loss: 0.3053 / \n",
      "Epoch 85 Avg Train Loss: 0.2753 Avg Valid Loss: 0.3284 / \n",
      "Epoch 86 Avg Train Loss: 0.2757 Avg Valid Loss: 0.3299 / \n",
      "Epoch 87 Avg Train Loss: 0.2769 Avg Valid Loss: 0.3225 / \n",
      "Epoch 88 Avg Train Loss: 0.2761 Avg Valid Loss: 0.3426 / \n",
      "Epoch 89 Avg Train Loss: 0.2755 Avg Valid Loss: 0.3165 / \n",
      "Epoch 90 Avg Train Loss: 0.2699 Avg Valid Loss: 0.3145 / \n",
      "Epoch 91 Avg Train Loss: 0.2756 Avg Valid Loss: 0.3175 / \n",
      "Epoch 92 Avg Train Loss: 0.2651 Avg Valid Loss: 0.3169 / \n",
      "Epoch 93 Avg Train Loss: 0.2706 Avg Valid Loss: 0.3187 / \n",
      "Epoch 94 Avg Train Loss: 0.2702 Avg Valid Loss: 0.3183 / \n",
      "Epoch 95 Avg Train Loss: 0.2748 Avg Valid Loss: 0.3180 / \n",
      "Epoch 96 Avg Train Loss: 0.2650 Avg Valid Loss: 0.3136 / \n",
      "Epoch 97 Avg Train Loss: 0.2724 Avg Valid Loss: 0.3175 / \n",
      "Epoch 98 Avg Train Loss: 0.2669 Avg Valid Loss: 0.3147 / \n",
      "Epoch 99 Avg Train Loss: 0.2716 Avg Valid Loss: 0.3141 / \n",
      "Epoch 100 Avg Train Loss: 0.2760 Avg Valid Loss: 0.3134 / \n",
      "========== fold: 3 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3025\n",
      "========== stage: 1 fold: 4 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5539 Avg Valid Loss: 0.5068 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5068\n",
      "Epoch 2 Avg Train Loss: 0.4690 Avg Valid Loss: 0.4325 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4325\n",
      "Epoch 3 Avg Train Loss: 0.4499 Avg Valid Loss: 0.4197 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4197\n",
      "Epoch 4 Avg Train Loss: 0.4366 Avg Valid Loss: 0.4160 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4160\n",
      "Epoch 5 Avg Train Loss: 0.4271 Avg Valid Loss: 0.4121 / \n",
      "Epoch 5 Save Best Valid Loss: 0.4121\n",
      "Epoch 6 Avg Train Loss: 0.4186 Avg Valid Loss: 0.4135 / \n",
      "Epoch 7 Avg Train Loss: 0.4088 Avg Valid Loss: 0.4060 / \n",
      "Epoch 7 Save Best Valid Loss: 0.4060\n",
      "Epoch 8 Avg Train Loss: 0.4127 Avg Valid Loss: 0.4052 / \n",
      "Epoch 8 Save Best Valid Loss: 0.4052\n",
      "Epoch 9 Avg Train Loss: 0.4033 Avg Valid Loss: 0.3971 / \n",
      "Epoch 9 Save Best Valid Loss: 0.3971\n",
      "Epoch 10 Avg Train Loss: 0.4016 Avg Valid Loss: 0.3902 / \n",
      "Epoch 10 Save Best Valid Loss: 0.3902\n",
      "Epoch 11 Avg Train Loss: 0.3926 Avg Valid Loss: 0.3896 / \n",
      "Epoch 11 Save Best Valid Loss: 0.3896\n",
      "Epoch 12 Avg Train Loss: 0.3921 Avg Valid Loss: 0.3957 / \n",
      "Epoch 13 Avg Train Loss: 0.3846 Avg Valid Loss: 0.3992 / \n",
      "Epoch 14 Avg Train Loss: 0.3844 Avg Valid Loss: 0.3928 / \n",
      "Epoch 15 Avg Train Loss: 0.3802 Avg Valid Loss: 0.3854 / \n",
      "Epoch 15 Save Best Valid Loss: 0.3854\n",
      "Epoch 16 Avg Train Loss: 0.3813 Avg Valid Loss: 0.3851 / \n",
      "Epoch 16 Save Best Valid Loss: 0.3851\n",
      "Epoch 17 Avg Train Loss: 0.3764 Avg Valid Loss: 0.3991 / \n",
      "Epoch 18 Avg Train Loss: 0.3774 Avg Valid Loss: 0.4072 / \n",
      "Epoch 19 Avg Train Loss: 0.3742 Avg Valid Loss: 0.3873 / \n",
      "Epoch 20 Avg Train Loss: 0.3753 Avg Valid Loss: 0.3843 / \n",
      "Epoch 20 Save Best Valid Loss: 0.3843\n",
      "Epoch 21 Avg Train Loss: 0.3683 Avg Valid Loss: 0.4013 / \n",
      "Epoch 22 Avg Train Loss: 0.3729 Avg Valid Loss: 0.3765 / \n",
      "Epoch 22 Save Best Valid Loss: 0.3765\n",
      "Epoch 23 Avg Train Loss: 0.3680 Avg Valid Loss: 0.3863 / \n",
      "Epoch 24 Avg Train Loss: 0.3656 Avg Valid Loss: 0.3929 / \n",
      "Epoch 25 Avg Train Loss: 0.3640 Avg Valid Loss: 0.3761 / \n",
      "Epoch 25 Save Best Valid Loss: 0.3761\n",
      "Epoch 26 Avg Train Loss: 0.3665 Avg Valid Loss: 0.4028 / \n",
      "Epoch 27 Avg Train Loss: 0.3610 Avg Valid Loss: 0.3885 / \n",
      "Epoch 28 Avg Train Loss: 0.3631 Avg Valid Loss: 0.4034 / \n",
      "Epoch 29 Avg Train Loss: 0.3587 Avg Valid Loss: 0.3707 / \n",
      "Epoch 29 Save Best Valid Loss: 0.3707\n",
      "Epoch 30 Avg Train Loss: 0.3588 Avg Valid Loss: 0.3851 / \n",
      "Epoch 31 Avg Train Loss: 0.3594 Avg Valid Loss: 0.3863 / \n",
      "Epoch 32 Avg Train Loss: 0.3557 Avg Valid Loss: 0.3938 / \n",
      "Epoch 33 Avg Train Loss: 0.3592 Avg Valid Loss: 0.4066 / \n",
      "Epoch 34 Avg Train Loss: 0.3581 Avg Valid Loss: 0.3896 / \n",
      "Epoch 35 Avg Train Loss: 0.3532 Avg Valid Loss: 0.3914 / \n",
      "Epoch 36 Avg Train Loss: 0.3538 Avg Valid Loss: 0.3930 / \n",
      "Epoch 37 Avg Train Loss: 0.3524 Avg Valid Loss: 0.3870 / \n",
      "Epoch 38 Avg Train Loss: 0.3510 Avg Valid Loss: 0.3919 / \n",
      "Epoch 39 Avg Train Loss: 0.3493 Avg Valid Loss: 0.4033 / \n",
      "Epoch 40 Avg Train Loss: 0.3499 Avg Valid Loss: 0.3707 / \n",
      "Epoch 40 Save Best Valid Loss: 0.3707\n",
      "Epoch 41 Avg Train Loss: 0.3480 Avg Valid Loss: 0.3969 / \n",
      "Epoch 42 Avg Train Loss: 0.3492 Avg Valid Loss: 0.3712 / \n",
      "Epoch 43 Avg Train Loss: 0.3423 Avg Valid Loss: 0.3718 / \n",
      "Epoch 44 Avg Train Loss: 0.3486 Avg Valid Loss: 0.3868 / \n",
      "Epoch 45 Avg Train Loss: 0.3455 Avg Valid Loss: 0.3781 / \n",
      "Epoch 46 Avg Train Loss: 0.3490 Avg Valid Loss: 0.3704 / \n",
      "Epoch 46 Save Best Valid Loss: 0.3704\n",
      "Epoch 47 Avg Train Loss: 0.3453 Avg Valid Loss: 0.3719 / \n",
      "Epoch 48 Avg Train Loss: 0.3478 Avg Valid Loss: 0.4064 / \n",
      "Epoch 49 Avg Train Loss: 0.3470 Avg Valid Loss: 0.3790 / \n",
      "Epoch 50 Avg Train Loss: 0.3484 Avg Valid Loss: 0.4005 / \n",
      "========== fold: 4 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.3704\n",
      "========== stage: 2 fold: 4 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.4002 Avg Valid Loss: 0.3257 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3257\n",
      "Epoch 2 Avg Train Loss: 0.3688 Avg Valid Loss: 0.3286 / \n",
      "Epoch 3 Avg Train Loss: 0.3577 Avg Valid Loss: 0.3190 / \n",
      "Epoch 3 Save Best Valid Loss: 0.3190\n",
      "Epoch 4 Avg Train Loss: 0.3456 Avg Valid Loss: 0.3119 / \n",
      "Epoch 4 Save Best Valid Loss: 0.3119\n",
      "Epoch 5 Avg Train Loss: 0.3413 Avg Valid Loss: 0.3381 / \n",
      "Epoch 6 Avg Train Loss: 0.3412 Avg Valid Loss: 0.3066 / \n",
      "Epoch 6 Save Best Valid Loss: 0.3066\n",
      "Epoch 7 Avg Train Loss: 0.3322 Avg Valid Loss: 0.3270 / \n",
      "Epoch 8 Avg Train Loss: 0.3352 Avg Valid Loss: 0.3191 / \n",
      "Epoch 9 Avg Train Loss: 0.3338 Avg Valid Loss: 0.3197 / \n",
      "Epoch 10 Avg Train Loss: 0.3324 Avg Valid Loss: 0.3197 / \n",
      "Epoch 11 Avg Train Loss: 0.3313 Avg Valid Loss: 0.3317 / \n",
      "Epoch 12 Avg Train Loss: 0.3291 Avg Valid Loss: 0.3250 / \n",
      "Epoch 13 Avg Train Loss: 0.3297 Avg Valid Loss: 0.3211 / \n",
      "Epoch 14 Avg Train Loss: 0.3290 Avg Valid Loss: 0.3201 / \n",
      "Epoch 15 Avg Train Loss: 0.3278 Avg Valid Loss: 0.3468 / \n",
      "Epoch 16 Avg Train Loss: 0.3284 Avg Valid Loss: 0.3363 / \n",
      "Epoch 17 Avg Train Loss: 0.3165 Avg Valid Loss: 0.3116 / \n",
      "Epoch 18 Avg Train Loss: 0.3242 Avg Valid Loss: 0.3118 / \n",
      "Epoch 19 Avg Train Loss: 0.3154 Avg Valid Loss: 0.3060 / \n",
      "Epoch 19 Save Best Valid Loss: 0.3060\n",
      "Epoch 20 Avg Train Loss: 0.3140 Avg Valid Loss: 0.3271 / \n",
      "Epoch 21 Avg Train Loss: 0.3180 Avg Valid Loss: 0.3277 / \n",
      "Epoch 22 Avg Train Loss: 0.3105 Avg Valid Loss: 0.3206 / \n",
      "Epoch 23 Avg Train Loss: 0.3188 Avg Valid Loss: 0.3251 / \n",
      "Epoch 24 Avg Train Loss: 0.3078 Avg Valid Loss: 0.3148 / \n",
      "Epoch 25 Avg Train Loss: 0.3204 Avg Valid Loss: 0.3364 / \n",
      "Epoch 26 Avg Train Loss: 0.3114 Avg Valid Loss: 0.3290 / \n",
      "Epoch 27 Avg Train Loss: 0.3067 Avg Valid Loss: 0.3138 / \n",
      "Epoch 28 Avg Train Loss: 0.3093 Avg Valid Loss: 0.3247 / \n",
      "Epoch 29 Avg Train Loss: 0.3130 Avg Valid Loss: 0.3461 / \n",
      "Epoch 30 Avg Train Loss: 0.3119 Avg Valid Loss: 0.3578 / \n",
      "Epoch 31 Avg Train Loss: 0.3169 Avg Valid Loss: 0.3360 / \n",
      "Epoch 32 Avg Train Loss: 0.3062 Avg Valid Loss: 0.3203 / \n",
      "Epoch 33 Avg Train Loss: 0.3085 Avg Valid Loss: 0.3555 / \n",
      "Epoch 34 Avg Train Loss: 0.3054 Avg Valid Loss: 0.3214 / \n",
      "Epoch 35 Avg Train Loss: 0.3005 Avg Valid Loss: 0.3220 / \n",
      "Epoch 36 Avg Train Loss: 0.3095 Avg Valid Loss: 0.3305 / \n",
      "Epoch 37 Avg Train Loss: 0.3081 Avg Valid Loss: 0.3319 / \n",
      "Epoch 38 Avg Train Loss: 0.3040 Avg Valid Loss: 0.3147 / \n",
      "Epoch 39 Avg Train Loss: 0.3028 Avg Valid Loss: 0.3128 / \n",
      "Epoch 40 Avg Train Loss: 0.3051 Avg Valid Loss: 0.3180 / \n",
      "Epoch 41 Avg Train Loss: 0.2994 Avg Valid Loss: 0.3234 / \n",
      "Epoch 42 Avg Train Loss: 0.3026 Avg Valid Loss: 0.3214 / \n",
      "Epoch 43 Avg Train Loss: 0.3026 Avg Valid Loss: 0.3477 / \n",
      "Epoch 44 Avg Train Loss: 0.3017 Avg Valid Loss: 0.3197 / \n",
      "Epoch 45 Avg Train Loss: 0.3026 Avg Valid Loss: 0.3256 / \n",
      "Epoch 46 Avg Train Loss: 0.3019 Avg Valid Loss: 0.3167 / \n",
      "Epoch 47 Avg Train Loss: 0.2934 Avg Valid Loss: 0.3170 / \n",
      "Epoch 48 Avg Train Loss: 0.2953 Avg Valid Loss: 0.3325 / \n",
      "Epoch 49 Avg Train Loss: 0.3004 Avg Valid Loss: 0.3193 / \n",
      "Epoch 50 Avg Train Loss: 0.2966 Avg Valid Loss: 0.3160 / \n",
      "Epoch 51 Avg Train Loss: 0.2939 Avg Valid Loss: 0.3099 / \n",
      "Epoch 52 Avg Train Loss: 0.3006 Avg Valid Loss: 0.3437 / \n",
      "Epoch 53 Avg Train Loss: 0.2972 Avg Valid Loss: 0.3256 / \n",
      "Epoch 54 Avg Train Loss: 0.2962 Avg Valid Loss: 0.3161 / \n",
      "Epoch 55 Avg Train Loss: 0.2915 Avg Valid Loss: 0.3177 / \n",
      "Epoch 56 Avg Train Loss: 0.2954 Avg Valid Loss: 0.3200 / \n",
      "Epoch 57 Avg Train Loss: 0.2982 Avg Valid Loss: 0.3244 / \n",
      "Epoch 58 Avg Train Loss: 0.2964 Avg Valid Loss: 0.3134 / \n",
      "Epoch 59 Avg Train Loss: 0.2933 Avg Valid Loss: 0.3211 / \n",
      "Epoch 60 Avg Train Loss: 0.3007 Avg Valid Loss: 0.3077 / \n",
      "Epoch 61 Avg Train Loss: 0.3022 Avg Valid Loss: 0.3229 / \n",
      "Epoch 62 Avg Train Loss: 0.3032 Avg Valid Loss: 0.3196 / \n",
      "Epoch 63 Avg Train Loss: 0.3030 Avg Valid Loss: 0.3216 / \n",
      "Epoch 64 Avg Train Loss: 0.3039 Avg Valid Loss: 0.3218 / \n",
      "Epoch 65 Avg Train Loss: 0.3002 Avg Valid Loss: 0.3276 / \n",
      "Epoch 66 Avg Train Loss: 0.3030 Avg Valid Loss: 0.3181 / \n",
      "Epoch 67 Avg Train Loss: 0.3022 Avg Valid Loss: 0.3251 / \n",
      "Epoch 68 Avg Train Loss: 0.2980 Avg Valid Loss: 0.3246 / \n",
      "Epoch 69 Avg Train Loss: 0.2985 Avg Valid Loss: 0.3244 / \n",
      "Epoch 70 Avg Train Loss: 0.3020 Avg Valid Loss: 0.3204 / \n",
      "Epoch 71 Avg Train Loss: 0.3009 Avg Valid Loss: 0.3256 / \n",
      "Epoch 72 Avg Train Loss: 0.3026 Avg Valid Loss: 0.3249 / \n",
      "Epoch 73 Avg Train Loss: 0.2999 Avg Valid Loss: 0.3240 / \n",
      "Epoch 74 Avg Train Loss: 0.3009 Avg Valid Loss: 0.3194 / \n",
      "Epoch 75 Avg Train Loss: 0.2989 Avg Valid Loss: 0.3244 / \n",
      "Epoch 76 Avg Train Loss: 0.2989 Avg Valid Loss: 0.3277 / \n",
      "Epoch 77 Avg Train Loss: 0.2995 Avg Valid Loss: 0.3220 / \n",
      "Epoch 78 Avg Train Loss: 0.3011 Avg Valid Loss: 0.3230 / \n",
      "Epoch 79 Avg Train Loss: 0.2993 Avg Valid Loss: 0.3238 / \n",
      "Epoch 80 Avg Train Loss: 0.3074 Avg Valid Loss: 0.3239 / \n",
      "Epoch 81 Avg Train Loss: 0.2983 Avg Valid Loss: 0.3202 / \n",
      "Epoch 82 Avg Train Loss: 0.3025 Avg Valid Loss: 0.3187 / \n",
      "Epoch 83 Avg Train Loss: 0.3008 Avg Valid Loss: 0.3211 / \n",
      "Epoch 84 Avg Train Loss: 0.2968 Avg Valid Loss: 0.3192 / \n",
      "Epoch 85 Avg Train Loss: 0.2977 Avg Valid Loss: 0.3206 / \n",
      "Epoch 86 Avg Train Loss: 0.3018 Avg Valid Loss: 0.3264 / \n",
      "Epoch 87 Avg Train Loss: 0.3033 Avg Valid Loss: 0.3261 / \n",
      "Epoch 88 Avg Train Loss: 0.2985 Avg Valid Loss: 0.3251 / \n",
      "Epoch 89 Avg Train Loss: 0.3010 Avg Valid Loss: 0.3202 / \n",
      "Epoch 90 Avg Train Loss: 0.2956 Avg Valid Loss: 0.3199 / \n",
      "Epoch 91 Avg Train Loss: 0.2947 Avg Valid Loss: 0.3189 / \n",
      "Epoch 92 Avg Train Loss: 0.3078 Avg Valid Loss: 0.3265 / \n",
      "Epoch 93 Avg Train Loss: 0.3020 Avg Valid Loss: 0.3241 / \n",
      "Epoch 94 Avg Train Loss: 0.3037 Avg Valid Loss: 0.3257 / \n",
      "Epoch 95 Avg Train Loss: 0.2994 Avg Valid Loss: 0.3229 / \n",
      "Epoch 96 Avg Train Loss: 0.2981 Avg Valid Loss: 0.3228 / \n",
      "Epoch 97 Avg Train Loss: 0.3026 Avg Valid Loss: 0.3203 / \n",
      "Epoch 98 Avg Train Loss: 0.2983 Avg Valid Loss: 0.3239 / \n",
      "Epoch 99 Avg Train Loss: 0.2975 Avg Valid Loss: 0.3221 / \n",
      "Epoch 100 Avg Train Loss: 0.3035 Avg Valid Loss: 0.3207 / \n",
      "========== fold: 4 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3060\n",
      "============ CV score with best loss weights ============\n",
      "Stage 0: 0.3724\n",
      "============ CV score with best loss weights ============\n",
      "Stage 1: 0.3039\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" and CFG.train_by_folds:\n",
    "    seed_torch(seed=CFG.seed)\n",
    "\n",
    "    stages_scores = {i: [] for i in CFG.train_stages}\n",
    "    stages_oof_df = {i: pd.DataFrame() for i in CFG.train_stages}\n",
    "\n",
    "    for fold in CFG.train_folds:\n",
    "\n",
    "        prev_dir = \"\"\n",
    "        for stage in range(len(CFG.total_evaluators)):\n",
    "\n",
    "            pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "            if not os.path.exists(pop_dir):\n",
    "                os.makedirs(pop_dir)\n",
    "\n",
    "            if stage not in CFG.train_stages:\n",
    "                prev_dir = pop_dir\n",
    "                continue\n",
    "\n",
    "            train_oof_df, score = train_loop(\n",
    "                stage=stage + 1,\n",
    "                epochs=CFG.epochs[stage],\n",
    "                fold=fold,\n",
    "                folds=train_pops[stage],\n",
    "                directory=pop_dir,\n",
    "                prev_dir=prev_dir,\n",
    "                eggs=all_eegs,\n",
    "            )\n",
    "\n",
    "            stages_oof_df[stage] = pd.concat([stages_oof_df[stage], train_oof_df])\n",
    "            stages_scores[stage].append(score)\n",
    "\n",
    "            prev_dir = pop_dir\n",
    "\n",
    "            LOGGER.info(f\"========== fold: {fold} stage: {stage+1} result ==========\")\n",
    "            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n",
    "\n",
    "    for stage, scores in stages_scores.items():\n",
    "        LOGGER.info(f\"============ CV score with best loss weights ============\")\n",
    "        LOGGER.info(f\"Stage {stage}: {np.mean(scores):.4f}\")\n",
    "\n",
    "    for stage, oof_df in stages_oof_df.items():\n",
    "        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "        oof_df.reset_index(drop=True, inplace=True)\n",
    "        oof_df.to_csv(\n",
    "            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168c182",
   "metadata": {
    "papermill": {
     "duration": 0.083593,
     "end_time": "2024-04-04T18:03:43.684916",
     "exception": false,
     "start_time": "2024-04-04T18:03:43.601323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a2e2ecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T18:03:43.855066Z",
     "iopub.status.busy": "2024-04-04T18:03:43.854574Z",
     "iopub.status.idle": "2024-04-04T18:03:43.905178Z",
     "shell.execute_reply": "2024-04-04T18:03:43.904099Z"
    },
    "papermill": {
     "duration": 0.138419,
     "end_time": "2024-04-04T18:03:43.907128",
     "exception": false,
     "start_time": "2024-04-04T18:03:43.768709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score with resnet1D_gru Raw EEG = 0.3039\n"
     ]
    }
   ],
   "source": [
    "# === Pre-process OOF ===\n",
    "gt = oof_df[[\"eeg_id\"] + CFG.target_cols]\n",
    "gt.sort_values(by=\"eeg_id\", inplace=True)\n",
    "gt.reset_index(inplace=True, drop=True)\n",
    "\n",
    "preds = oof_df[[\"eeg_id\"] + CFG.pred_cols]\n",
    "preds.columns = [\"eeg_id\"] + CFG.target_cols\n",
    "preds.sort_values(by=\"eeg_id\", inplace=True)\n",
    "preds.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_trues = gt[CFG.target_cols]\n",
    "y_preds = preds[CFG.target_cols]\n",
    "\n",
    "oof = pd.DataFrame(y_preds.copy())\n",
    "oof[\"id\"] = np.arange(len(oof))\n",
    "\n",
    "true = pd.DataFrame(y_trues.copy())\n",
    "true[\"id\"] = np.arange(len(true))\n",
    "\n",
    "cv = kaggle_kl_div.score(solution=true, submission=oof, row_id_column_name=\"id\")\n",
    "print(f\"CV Score with resnet1D_gru Raw EEG = {cv:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 4297749,
     "sourceId": 7392733,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4317718,
     "sourceId": 7465251,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4378712,
     "sourceId": 7517324,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17084.814987,
   "end_time": "2024-04-04T18:03:46.966343",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-04T13:19:02.151356",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "088fc3f5e4264683bbeaa2d929fab599": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2c3d333449fb412dbc86ca31835ef4b0",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3ba5491929cf4cb1bbb8e28514933aa6",
       "value": 1
      }
     },
     "11bfd3544cf8411e96cd7ede1451518c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5246ec5881c147f09c90846e9911dda6",
       "placeholder": "​",
       "style": "IPY_MODEL_db8fdfb810194e03b66c7c5e5077ccf5",
       "value": " 1/? [00:00&lt;00:00,  1.37it/s]"
      }
     },
     "2c3d333449fb412dbc86ca31835ef4b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "3ba5491929cf4cb1bbb8e28514933aa6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5246ec5881c147f09c90846e9911dda6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58eb07f979fd49e8a9155ad60c77923c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "993085d825a542b1b5c44cd42f7f8c89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c818523154ec45689615162b4c03bf96",
        "IPY_MODEL_088fc3f5e4264683bbeaa2d929fab599",
        "IPY_MODEL_11bfd3544cf8411e96cd7ede1451518c"
       ],
       "layout": "IPY_MODEL_fa70a85db4944741a7d7b13cef19bb70"
      }
     },
     "c818523154ec45689615162b4c03bf96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_58eb07f979fd49e8a9155ad60c77923c",
       "placeholder": "​",
       "style": "IPY_MODEL_ea4dcc5f4fcf4011a7ba7dc587109a9b",
       "value": ""
      }
     },
     "db8fdfb810194e03b66c7c5e5077ccf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ea4dcc5f4fcf4011a7ba7dc587109a9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fa70a85db4944741a7d7b13cef19bb70": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
